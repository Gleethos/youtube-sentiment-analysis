{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Sentiment - Analyser #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data ##\n",
    "\n",
    "Before starting with the machine learning and sentiment analysis\n",
    "we first have to load the data and clean it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri, user, password = 'bolt://localhost:7687', 'neo4j', 'neo4j_'\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# resetting database\n",
    "\n",
    "words = []\n",
    "comments = []\n",
    "scores = []\n",
    "\n",
    "with driver.session() as session:\n",
    "    def _q(query) : return session.run(query)\n",
    "    #---------------------------------------\n",
    "\n",
    "    result = _q(\"MATCH (n:Author)-[c:COMMENTED]->(v:Video) RETURN n, c, v\") # remove all graphs and nodes! BE CAREFUL!\n",
    " \n",
    "    for record in result:\n",
    "        newComment = ((\"%s | %s\" % (record['v']['title'],record['c']['text'])).lower()\n",
    "            .replace('â¤ï¸', ' heart ').replace('ðŸ’¯', ' 100 ').replace('â¤', ' heart ')\n",
    "            .replace('ðŸ™', ' pray ').replace('ðŸ˜˜', ' kiss ').replace('ðŸ¤—', ' happy ')\n",
    "            .replace('ðŸ’¥', ' boom ').replace('âœ”ï¸', ' like ').replace('ðŸ˜', ' love ')\n",
    "            .replace('ðŸ±', ' cat ').replace('ðŸ’”', ' broken heart ').replace('ðŸ˜µ', ' confused ') \n",
    "            .replace('ðŸ˜„', ' awesome ').replace('ðŸ‘', ' thumbs up ').replace('ðŸ˜Ž', ' cool ')\n",
    "            .replace('ðŸ·', ' pig ').replace('ðŸ¤˜', \" rock'n roll \").replace('ðŸ¤£', ' laughing hard ')\n",
    "            .replace('ðŸ˜©', ' oh no ').replace('ðŸ’Ž', ' diamond ').replace('ðŸ˜Š', ' nice ')\n",
    "            .replace('â˜ºï¸', ' very nice ').replace('ðŸ™ƒ', ' upside down smile ').replace('ðŸ¤”', ' not sure ')\n",
    "            .replace('ðŸ˜‚', ' laughing ')\n",
    "            .replace('!!!!', '!').replace('!!!', '!').replace('!!', '!')\n",
    "            .replace('????', '?').replace('???', '?').replace('??', '?')\n",
    "            .replace('oooo','o').replace('oooo','o').replace('ooo','o').replace('oo','o')\n",
    "            .replace('..','...').replace('......','...').replace('....','...').replace('....','...').replace('...',' ... ')\n",
    "            .replace('    ', ' ').replace('   ', ' ').replace('  ', ' ')\n",
    "            .split() \n",
    "        )\n",
    "        words.extend(newComment)\n",
    "        comments.append(newComment)\n",
    "        scores.append(float(record['c']['score']))\n",
    "    \n",
    "    #---------------------------------------\n",
    "driver.close()\n",
    "\n",
    "#print(comments)\n",
    "#print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's clean the text with spark ##\n",
    "\n",
    "The comments are currently still full of redundancies and\n",
    "many uncommon tokens / word.\n",
    "In order to be able to filter the comments we use spark to count their occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    " \n",
    "sc = SparkContext(\"local\", \"First App\")\n",
    " \n",
    "wordsRDD = sc.parallelize(words, 6)\n",
    "\n",
    "zipped = (wordsRDD\n",
    "                       .map(lambda word : (word,1))\n",
    "                       .reduceByKey( lambda a, b : a+b )\n",
    "                       .collect())\n",
    "sc.stop()                  \n",
    "\n",
    "occurrences = {}\n",
    "\n",
    "for word, count in zipped:\n",
    "    occurrences[word] = count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and now let's remove rare tokens / words in order to cap the maximum amount of possible words a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def less_than_three(word):\n",
    "    return occurrences[word] > 3\n",
    "\n",
    "for i, comment in enumerate(comments): \n",
    "    comments[i] = list(filter(less_than_three, comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sentiment', 'analysis', 'python', '-', '1', '-', 'introduction', 'to', 'emotion', 'analysis', '(nlp)', '|', 'great', 'series'], ['sentiment', 'analysis', 'python', '-', '1', '-', 'introduction', 'to', 'emotion', 'analysis', '(nlp)', '|', 'is'], ['sentiment', 'analysis', 'python', '-', '1', '-', 'introduction', 'to', 'emotion', 'analysis', '(nlp)', '|', 'could', 'we', 'have', 'the', 'code'], ['sentiment', 'analysis', 'python', '-', '1', '-', 'introduction', 'to', 'emotion', 'analysis', '(nlp)', '|', 'please', '2', '3', 'videos'], ['sentiment', 'analysis', 'python', '-', '1', '-', 'introduction', 'to', 'emotion', 'analysis', '(nlp)', '|', 'your', 'is', 'to', 'more', 'videos', 'on', 'python'], ['sentiment', 'analysis', 'python', '-', '1', '-', 'introduction', 'to', 'emotion', 'analysis', '(nlp)', '|', 'is'], ['sentiment', 'analysis', 'python', '-', '1', '-', 'introduction', 'to', 'emotion', 'analysis', '(nlp)', '|', 'can', 'please', 'give', 'me', 'code', 'for', 'different', 'emotions', 'like', 'and', 'from', 'of'], ['sentiment', 'analysis', 'python', '-', '1', '-', 'introduction', 'to', 'emotion', 'analysis', '(nlp)', '|', 'love', 'the'], ['sentiment', 'analysis', 'python', '-', '1', '-', 'introduction', 'to', 'emotion', 'analysis', '(nlp)', '|', 'please', 'can', 'u', 'make', 'a', 'of'], ['sentiment', 'analysis', 'python', '-', '1', '-', 'introduction', 'to', 'emotion', 'analysis', '(nlp)', '|', 'really', 'by', 'your', 'emotions'], ['sentiment', 'analysis', 'python', '-', '2', '-', 'creating', 'project', 'and', 'installing', 'python', '(nlp)', '|', 'could', 'you', 'code', 'next', 'time', 'much'], ['sentiment', 'analysis', 'python', '-', '2', '-', 'creating', 'project', 'and', 'installing', 'python', '(nlp)', '|', 'can', 'i', 'use', 'or', 'for', 'this'], ['sentiment', 'analysis', 'python', '-', '2', '-', 'creating', 'project', 'and', 'installing', 'python', '(nlp)', '|', 'nice'], ['sentiment', 'analysis', 'python', '-', '3', '-', 'cleaning', 'text', 'for', 'natural', 'language', 'processing', '(nlp)', '|', 'keep', 'up', 'the', 'work', 'man', 'we', 'love', 'the', 'of', 'the', 'video', 'you', 'what', 'it', 'was', 'really', 'nice', 'i', 'really', 'to', '2', 'and', 'how', 'it', 'to', 'the', 'please', 'keep', 'just', 'a', 'in', 'when', 'you', 'what', 'you', 'you', 'really', 'like', 'you', 'something', 'and', 'not', 'just', 'what', 'a'], ['sentiment', 'analysis', 'python', '-', '3', '-', 'cleaning', 'text', 'for', 'natural', 'language', 'processing', '(nlp)', '|', 'i', 'just', 'love', 'u', '...', 'i', 'from', 'u', 'and', 'now', 'i', 'am', 'nltk', '...', 'nice', '...', 'u', 'know', 'very', 'what', 'to', '...', 'how', '...', 'thank', 'you', 'so', 'much', '...', 'love'], ['sentiment', 'analysis', 'python', '-', '3', '-', 'cleaning', 'text', 'for', 'natural', 'language', 'processing', '(nlp)', '|', 'bro', 'please', 'some', 'python', 'project', 'video', 'please'], ['sentiment', 'analysis', 'python', '-', '3', '-', 'cleaning', 'text', 'for', 'natural', 'language', 'processing', '(nlp)', '|', 'how', 'are', 'text', 'into', 'error', 'has'], ['sentiment', 'analysis', 'python', '-', '3', '-', 'cleaning', 'text', 'for', 'natural', 'language', 'processing', '(nlp)', '|', 'can', 'u', 'make', 'video', 'on', 'of', 'text', 'using', 'python', '...', '...', ',', '...', 'from', 'the', 'text', 'file'], ['sentiment', 'analysis', 'python', '-', '3', '-', 'cleaning', 'text', 'for', 'natural', 'language', 'processing', '(nlp)', '|', 'i', 'data', '='], ['sentiment', 'analysis', 'python', '-', '3', '-', 'cleaning', 'text', 'for', 'natural', 'language', 'processing', '(nlp)', '|', 'has'], ['sentiment', 'analysis', 'python', '-', '3', '-', 'cleaning', 'text', 'for', 'natural', 'language', 'processing', '(nlp)', '|', 'you', 'it', 'in', 'an', '...'], ['sentiment', 'analysis', 'python', '-', '3', '-', 'cleaning', 'text', 'for', 'natural', 'language', 'processing', '(nlp)', '|', 'so', 'to', 'code', 'this', 'this', 'is', 'just', 'what', 'i', 'to', 'my', 'python', 'my', 'to', 'code', 'a'], ['sentiment', 'analysis', 'python', '-', '3', '-', 'cleaning', 'text', 'for', 'natural', 'language', 'processing', '(nlp)', '|', 'hey', 'man', 'time'], ['sentiment', 'analysis', 'python', '-', '3', '-', 'cleaning', 'text', 'for', 'natural', 'language', 'processing', '(nlp)', '|', 'thank', 'you', '...'], ['sentiment', 'analysis', 'python', '-', '3', '-', 'cleaning', 'text', 'for', 'natural', 'language', 'processing', '(nlp)', '|', 'great', 'video', ',', 'keep', 'this', 'up', 'bro'], ['sentiment', 'analysis', 'python', '-', '4', '-', 'tokenization', 'and', 'stop', 'words', '(nlp)', '|', 'u', 'use', 'stop', 'words'], ['sentiment', 'analysis', 'python', '-', '4', '-', 'tokenization', 'and', 'stop', 'words', '(nlp)', '|', 'bro', '...', '...', 'videos', 'a', 'lot', 'man', '...', 'thanks', '...', 'waiting', 'for', 'your', 'next', 'videos'], ['sentiment', 'analysis', 'python', '-', '4', '-', 'tokenization', 'and', 'stop', 'words', '(nlp)', '|', 'this', 'me', 'god', 'of', '...', '...'], ['sentiment', 'analysis', 'python', '-', '4', '-', 'tokenization', 'and', 'stop', 'words', '(nlp)', '|', 'keep', 'up', 'the', 'god'], ['sentiment', 'analysis', 'python', '-', '4', '-', 'tokenization', 'and', 'stop', 'words', '(nlp)', '|', 'thank', 'you', 'very', 'much', 'for'], ['sentiment', 'analysis', 'python', '-', '4', '-', 'tokenization', 'and', 'stop', 'words', '(nlp)', '|'], ['sentiment', 'analysis', 'python', '-', '4', '-', 'tokenization', 'and', 'stop', 'words', '(nlp)', '|', 'to', 'this', 'in', 'the'], ['sentiment', 'analysis', 'python', '-', '4', '-', 'tokenization', 'and', 'stop', 'words', '(nlp)', '|', 'love', 'keep', 'it'], ['sentiment', 'analysis', 'python', '-', '4', '-', 'tokenization', 'and', 'stop', 'words', '(nlp)', '|', 'i', 'am', 'to', 'make', 'a', 'project', 'on', 'this', '...', 'it', 'that', 'your', 'video', 'series', 'will', 'help', 'in', 'this', 'very', 'nice', 'i', 'more', 'on', 'this', 'of', 'analysis'], ['sentiment', 'analysis', 'python', '-', '4', '-', 'tokenization', 'and', 'stop', 'words', '(nlp)', '|', 'that', 'such', 'words', 'as', 'be', 'can', 'be', 'a', 'of', 'as', 'in', 'am', 'really'], ['sentiment', 'analysis', 'python', '-', '4', '-', 'tokenization', 'and', 'stop', 'words', '(nlp)', '|', 'thank', 'you', 'so', 'much!', 'i', 'really', 'love', 'your', 'of', 'not', 'using', 'nltk', 'or', 'but', 'the', 'thank', 'you', 'so', 'much!', 'i', 'can', 'it'], ['sentiment', 'analysis', 'python', '-', '4', '-', 'tokenization', 'and', 'stop', 'words', '(nlp)', '|', 'i', 'am', 'really', 'with', 'all', 'videos', 'of', 'and', 'of', 'but', 'waiting', 'for', 'the', 'next', 'thanks', 'man', '...'], ['sentiment', 'analysis', 'python', '-', '5', '-', 'algorithm', 'for', 'emotion', 'and', 'text', 'analysis', '(nlp)', '|', 'error', 'on'], ['sentiment', 'analysis', 'python', '-', '5', '-', 'algorithm', 'for', 'emotion', 'and', 'text', 'analysis', '(nlp)', '|', 'what', 'if', 'i', 'want', 'emotions', 'in', 'different'], ['sentiment', 'analysis', 'python', '-', '5', '-', 'algorithm', 'for', 'emotion', 'and', 'text', 'analysis', '(nlp)', '|', 'i', 'had', 'a', 'this', 'code'], ['sentiment', 'analysis', 'python', '-', '5', '-', 'algorithm', 'for', 'emotion', 'and', 'text', 'analysis', '(nlp)', '|', 'how', 'you', 'or', 'create', 'the', 'emotions', 'been', 'the', 'to', 'create', 'like', 'that', 'but', 'have', 'with', 'for', 'of'], ['sentiment', 'analysis', 'python', '-', '5', '-', 'algorithm', 'for', 'emotion', 'and', 'text', 'analysis', '(nlp)', '|', 'can', 'this', 'be', 'in'], ['sentiment', 'analysis', 'python', '-', '5', '-', 'algorithm', 'for', 'emotion', 'and', 'text', 'analysis', '(nlp)', '|', 'nice', 'video', 'was', 'the', 'i', 'in', 'my', 'it', 'when', 'things', 'my', 'had', 'a', 'of', 'creating', 'an', 'be', 'to', 'emotions', 'like', 'and', 'and', 'and', 'on', 'that', 'i', 'had', 'got', 'this', 'on', 'time'], ['sentiment', 'analysis', 'python', '-', '5', '-', 'algorithm', 'for', 'emotion', 'and', 'text', 'analysis', '(nlp)', '|', 'very', 'videos', 'but', 'i', 'really', 'want', 'to', 'know', 'how', 'you', 'get', 'the', 'emotion'], ['sentiment', 'analysis', 'python', '-', '6', '-', 'counting', 'emotions', '-', 'natural', 'language', 'processing', '(nlp)', '|', 'can', 'you', 'use', 'the', 'for', 'file', '?'], ['sentiment', 'analysis', 'python', '-', '6', '-', 'counting', 'emotions', '-', 'natural', 'language', 'processing', '(nlp)', '|', 'how', 'you', 'will', 'into'], ['sentiment', 'analysis', 'python', '-', '6', '-', 'counting', 'emotions', '-', 'natural', 'language', 'processing', '(nlp)', '|', '=', 'not', 'to', 'got'], ['sentiment', 'analysis', 'python', '-', '6', '-', 'counting', 'emotions', '-', 'natural', 'language', 'processing', '(nlp)', '|', 'so', 'how', 'the', 'code', 'from', 'the', 'is', 'in', 'the'], ['sentiment', 'analysis', 'python', '-', '6', '-', 'counting', 'emotions', '-', 'natural', 'language', 'processing', '(nlp)', '|', 'i', 'data', 'and', 'analysis', 'but', 'in', 'my', 'text', 'file', 'not', 'to', 'what', 'i'], ['sentiment', 'analysis', 'python', '-', '6', '-', 'counting', 'emotions', '-', 'natural', 'language', 'processing', '(nlp)', '|', 'this', 'for', 'but', 'i', 'get', 'as', 'as', 'i', 'have', 'to', 'for', 'i', 'have', 'of', 'for', 'by', 'my', 'file', 'has', 'words', 'it', 'has', 'been', 'and', 'stop', 'words', 'have', 'been', 'but', 'emotions', 'are', 'is', 'words', 'of', 'i', 'a', 'emotion', 'file', 'or', 'i', 'be', 'something', 'but', 'i', 'a', 'lot', 'of', 'to'], ['sentiment', 'analysis', 'python', '-', '7', '-', 'emotions', 'in', 'a', 'graph', 'using', 'matplotlib', '(nlp)', '|', 'waiting', 'for', 'next', 'video', '...', '...', '...'], ['sentiment', 'analysis', 'python', '-', '7', '-', 'emotions', 'in', 'a', 'graph', 'using', 'matplotlib', '(nlp)', '|', 'hey', 'when', 'i', 'something', 'in', 'file', 'all', 'things', 'but', 'when', 'i', 'and', 'it', 'from', 'it', 'give', 'me', 'this', 'error', 'in', '1', 'now', 'what', 'i', 'do'], ['sentiment', 'analysis', 'python', '-', '7', '-', 'emotions', 'in', 'a', 'graph', 'using', 'matplotlib', '(nlp)', '|', 'i', 'have', 'a', 'when', 'i', 'file', 'in', 'the', 'and', 'to', 'the', 'graph', 'it', 'just', 'like', 'the', 'file', 'is', 'it', 'for', 'and', 'by'], ['sentiment', 'analysis', 'python', '-', '7', '-', 'emotions', 'in', 'a', 'graph', 'using', 'matplotlib', '(nlp)', '|', 'when', 'the', 'video', 'is'], ['sentiment', 'analysis', 'python', '-', '7', '-', 'emotions', 'in', 'a', 'graph', 'using', 'matplotlib', '(nlp)', '|', 'very', 'god', 'help', 'me', 'for', 'how', 'things', 'work', 'on', 'just', 'using', 'keep', 'the', 'god', 'work', 'god', 'you', 'and', 'your'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'this', 'is', 'very', 'is', 'it', 'to', 'get', 'a', 'emotion', 'for', 'a', 'as', 'to', 'getting', 'an', 'emotion', 'in', 'the'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'how', 'do', 'i', 'the', 'tweets', 'into', 'a', 'in', 'a', 'text'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'great', 'thank', 'you', 'so', 'i', 'was', 'how', 'do', 'you', 'it', 'into', 'a'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'been', 'for', 'this', 'for', 'a'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'nice', 'video', 'but', 'i', 'am', 'using', 'am', 'getting', 'to'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'thank', 'you', 'very', 'much!', 'is', 'of', 'the', 'videos', 'i', 'have', 'so', 'i', 'to', 'if', 'you', 'have', 'the', 'of', 'code', 'to', 'get', 'the', 'emotions', 'and', 'the', 'thank'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'how', 'tweets', 'can', 'i', 'get', 'from', 'this', 'i', 'you', 'are', 'using', 'for', '10', 'tweets', 'if', 'i', 'want', 'to', 'all', 'the', 'tweets', 'the', 'time', 'what', 'i', 'have', 'to', '?'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'not', 'to', 'get', 'tweets', 'what', 'is', 'to', 'get', 'tweets', 'we', 'to', 'have', 'an', 'and', '...', '?'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'video', 'got', 'a', 'in'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'very', 'god', 'getoldtweets3', 'to', 'be', 'very', 'to', 'but', 'is', 'it', 'a', 'to', 'get', 'tweets', 'some', 'of', 'time', 'in', 'some', 'i', 'this'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'from', ',', 'using', 'this', 'are', 'getting', 'error', 'twitter', 'it'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'when', 'are', 'a', 'lot', 'of', 'emotions', 'when', 'the', 'tweets', 'are', 'the', 'words', 'are', 'in', 'the', 'graph', 'in', 'how', 'do', 'we', 'it', 'in', 'the', 'could', 'you', 'the', 'of', 'graph'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'to', 'in', 'in'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'your', 'like', 'a', 'but', 'i', 'know', 'what', 'i', 'have', 'to', 'to', 'get', 'an', 'is', 'not', 'can', 'you'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'to', '...', 'great', 'work', '...', 'i', 'will', 'next', 'also', '...'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'nice', 'but', 'to'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'thank', 'what', 'an', 'your', 'to', 'the', 'code', 'it', 'just', 'so', 'nice', 'bro'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'could', 'you', 'your'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'can', 'i', 'use', 'and', 'to', 'data', 'in', 'a', 'time', 'from', 'a', 'thanks', 'for', 'nice'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'an', 'error', 'an', 'error', 'not', 'me', 'this', 'error', 'please', 'help'], ['sentiment', 'analysis', 'python', '-', '8', '-', 'twitter', 'emotion', 'analysis', 'using', 'getoldtweets3', '|', 'hey', '...', 'i', 'have', 'all', 'the', 'videos', 'now', '...', 'thank', 'you', 'so', 'much', '...', 'not', 'are', 'me', 'in', 'my', 'project', '...', 'but', 'also', '...', 'getting', 'to', 'know', '...', '...', 'you', 'all'], ['sentiment', 'analysis', 'python', '-', '9', '-', 'installing', 'nltk', '|', 'tokenization', 'and', 'stop', 'words', '|', 'hey', 'can', 'you', 'please', 'how', 'to', 'this', 'to', 'using'], ['sentiment', 'analysis', 'python', '-', '9', '-', 'installing', 'nltk', '|', 'tokenization', 'and', 'stop', 'words', '|', 'if', 'your', 'nltk', 'a', 'error', 'you', 'can', 'use', 'nltk', '=', '='], ['sentiment', 'analysis', 'python', '-', '9', '-', 'installing', 'nltk', '|', 'tokenization', 'and', 'stop', 'words', '|', 'i', 'really', 'like', 'videos', 'you', 'do', 'a', 'great', 'in', 'different', 'if', 'i', 'more', 'videos', 'nltk', 'into', 'text'], ['sentiment', 'analysis', 'python', '-', '9', '-', 'installing', 'nltk', '|', 'tokenization', 'and', 'stop', 'words', '|', 'if', 'i', 'want', 'to', 'emotions', 'in', 'different', 'i', 'have', 'to', 'has', 'my', 'emotions', '?'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', '...', 'just', 'u', 'my', '...', 'also', 'to', 'give', 'the', 'of', 'the', 'me', 'a', 'lot', '...', 'that', 'cleaning', 'was', 'so'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'thank'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'thanks', 'for', 'videos', 'me', 'a', 'lot', '...', 'in', 'my', 'sentiment', 'analysis', '...', 'really', 'a'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'i', 'how', 'to', 'create', 'a', 'sentiment', 'analysis', 'by', 'videos', 'i', 'videos', 'on', 'this', ',', 'but', 'things', 'had', 'not', 'me', 'a', 'it', '...', 'but', 'now', 'i', 'got', 'it', '...', 'more', 'videos', 'u', 'will', 'be', 'helpful', 'more', 'like', 'me', '...'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'thank', 'you', 'so', 'helpful'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'bro', 'please', 'some', 'project', 'in', 'waiting'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'thank', 'i', 'you', 'it', 'my'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'god'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'how', 'to', 'the', 'i', 'have', 'a', 'and', 'data', 'as', 'my', 'data', 'now', 'to', 'that', 'this', 'is', 'the', 'or', 'or', 'something'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'i', 'to', 'sentiment', 'this', 'has', 'been', 'thank', 'you', 'so', 'much', 'for', 'such', 'a', 'great', 'video', 'please', 'keep', 'creating', 'such', 'great'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'do', 'with'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'bro', '...', 'can', 'u', 'make', '...', 'a', 'project', 'on', '...', 'that', 'of', 'analysis', '...', 'with', '...', 'that', '...', 'also', 'will', 'more', '...'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'very', 'videos', 'up'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'thank', 'you', 'very', 'much', 'for', 'very', 'helpful', 'please', 'more', 'such', 'as'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'very', 'helpful', 'for', 'more', 'videos'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'very', 'nice', 'work', 'can', 'you', 'create', 'a', 'for', 'twitter', 'analysis', 'using', 'python', 'by', 'using', 'twitter', 'nice', 'work', 'waiting', 'for', 'the', 'series'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'can', 'you', 'please', 'give', 'me', 'a', 'how', 'can', 'i', 'make', 'my', 'to', 'in', 'python'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'could', 'you', 'help', 'me', 'with', 'my', 'project', '?'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'how', 'do', 'you', 'get', 'the'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'thank', 'u', 'very', 'much', '...', 'it'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'thanks', 'the', 'sentiment', 'analysis', 'is', 'really', 'i', 'you', 'to', 'make', 'on', 'this', 'using', 'for'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'thanks', 'a', 'lot', '...', 'this', 'video', 'series', 'me', 'a', 'lot'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', '...', 'it', 'was', 'very', 'and'], ['sentiment', 'analysis', 'python', '-', '10', '-', 'positive', 'or', 'negative', 'sentiments', '|', 'nltk', '|', 'thank', 'you', 'very', 'much!', 'i', 'was', 'for', 'how', 'to', 'something', 'like', 'this', 'and', 'you', 'help', 'me', 'a', 'i', 'have', 'the', 'emotions', 'text', 'file', 'to', 'my', 'language', 'and', 'very', 'thank']]\n"
     ]
    }
   ],
   "source": [
    "print(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sentiment Analysis with Pytorch #\n",
    "\n",
    "Let's train a neural network on our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3ca05f1910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMScorePredictor(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_dim, do_softmax=True):\n",
    "        super(LSTMScorePredictor, self).__init__()\n",
    "        self.do_softmax = do_softmax\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.lin = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        out = self.lin(lstm_out.view(len(sentence), -1))\n",
    "        if self.do_softmax: out = F.log_softmax(out, dim=1)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the model: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Word to index encoding...\n",
    "class Encoder:\n",
    "    \n",
    "    def __init__(self, training_data):\n",
    "        self.word_to_ix = {}\n",
    "        # For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "        for sent, label in training_data:\n",
    "            for word in sent:\n",
    "                if word not in self.word_to_ix:  # word has not been assigned an index yet\n",
    "                    self.word_to_ix[word] = len(self.word_to_ix)  # Assign each word with a unique index\n",
    "        print(self.word_to_ix)\n",
    "        self.label_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
    "\n",
    "    def sequence_words(self, seq):\n",
    "        idxs = [self.word_to_ix[w] for w in seq]\n",
    "        return torch.tensor(idxs, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    def sequence_labels(self, seq):\n",
    "        idxs = [self.label_to_ix[w] for w in seq]\n",
    "        return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit-Testing the Model: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "tensor([[-0.8818, -1.0841, -1.3953],\n",
      "        [-0.9756, -0.9889, -1.3820],\n",
      "        [-0.9249, -1.0800, -1.3325],\n",
      "        [-0.8884, -1.1795, -1.2684],\n",
      "        [-0.8897, -1.1504, -1.2993]])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([[-0.0259, -4.5027, -4.2358],\n",
      "        [-4.5316, -0.0617, -3.0154],\n",
      "        [-2.6551, -2.9622, -0.1301],\n",
      "        [-0.1583, -3.9917, -2.0562],\n",
      "        [-4.2915, -0.0241, -4.5954]])\n"
     ]
    }
   ],
   "source": [
    "# Example training data:\n",
    "\n",
    "training_data = [ \n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "enc = Encoder(training_data)\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "model = LSTMScorePredictor(\n",
    "    embedding_dim=EMBEDDING_DIM, \n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    vocab_size=len(enc.word_to_ix), \n",
    "    output_dim=len(enc.label_to_ix) \n",
    ")\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = enc.sequence_words(training_data[0][0])\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "    \n",
    "assert str(tag_scores) == \"\"\"tensor([[-0.8818, -1.0841, -1.3953],\n",
    "        [-0.9756, -0.9889, -1.3820],\n",
    "        [-0.9249, -1.0800, -1.3325],\n",
    "        [-0.8884, -1.1795, -1.2684],\n",
    "        [-0.8897, -1.1504, -1.2993]])\"\"\"\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = enc.sequence_words(sentence)\n",
    "        targets = enc.sequence_labels(tags)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = enc.sequence_words(training_data[0][0])\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(inputs)\n",
    "    print(tag_scores)\n",
    "    \n",
    "assert str(inputs) == \"\"\"tensor([0, 1, 2, 3, 4])\"\"\"\n",
    "assert str(tag_scores) == \"\"\"tensor([[-0.0259, -4.5027, -4.2358],\n",
    "        [-4.5316, -0.0617, -3.0154],\n",
    "        [-2.6551, -2.9622, -0.1301],\n",
    "        [-0.1583, -3.9917, -2.0562],\n",
    "        [-4.2915, -0.0241, -4.5954]])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing some classical regression now! ##\n",
    "\n",
    "When evaluating the sentiment we want the result to be somethi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "tensor([[-0.1821],\n",
      "        [-0.1732],\n",
      "        [-0.2096],\n",
      "        [-0.2086],\n",
      "        [-0.1786]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukashinterleitner/anaconda3/envs/neo4j/lib/python3.8/site-packages/torch/nn/modules/loss.py:94: UserWarning: Using a target size (torch.Size([1, 4, 1])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([[-1.9960],\n",
      "        [ 3.0706],\n",
      "        [ 4.1011],\n",
      "        [-1.9683],\n",
      "        [ 6.1664]])\n",
      "tensor([5, 6, 7, 8])\n",
      "tensor([[ -5.1595],\n",
      "        [ -2.2969],\n",
      "        [ -0.2990],\n",
      "        [-11.1489]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2471408ca4eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m assert pred1 == \"\"\"tensor([[-1.9960],\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;34m[\u001b[0m \u001b[0;36m3.0707\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;34m[\u001b[0m \u001b[0;36m4.1012\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example training data:\n",
    "\n",
    "training_data = [ \n",
    "    (\"The dog ate the apple\".split(), torch.tensor([[-2], [3], [4], [-2], [6]])),\n",
    "    (\"Everybody read that book\".split(), torch.tensor([[[-5], [-2], [0], [-11]]]))\n",
    "]\n",
    "enc = Encoder(training_data)\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "model = LSTMScorePredictor(\n",
    "    embedding_dim=EMBEDDING_DIM, \n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    vocab_size=len(enc.word_to_ix), \n",
    "    output_dim=1,\n",
    "    do_softmax=False\n",
    ")\n",
    "loss_function = nn.L1Loss()# instead of nn.NLLLoss() (which is for softmax output...)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = enc.sequence_words(training_data[0][0])\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "    \n",
    " \n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = enc.sequence_words(sentence)\n",
    "        targets = tags# enc.sequence_labels(tags)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step() \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = enc.sequence_words(training_data[0][0])\n",
    "    pred1 = model(inputs)\n",
    " \n",
    "    print(inputs)\n",
    "    print(pred1)\n",
    "    \n",
    "    inputs = enc.sequence_words(training_data[1][0])\n",
    "    pred2 = model(inputs)\n",
    " \n",
    "    print(inputs)\n",
    "    print(pred2)\n",
    "     \n",
    "assert pred1 == \"\"\"tensor([[-1.9960],\n",
    "        [ 3.0707],\n",
    "        [ 4.1012],\n",
    "        [-1.9682],\n",
    "        [ 6.1665]])\"\"\"\n",
    "\n",
    "assert pred2 == \"\"\"tensor([[ -5.1595],\n",
    "        [ -2.2969],\n",
    "        [ -0.2990],\n",
    "        [-11.1489]])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
