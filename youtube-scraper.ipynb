{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Youtube - Scaper\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyppeteer import launch\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# for extended documentation visit --> https://miyakogi.github.io/pyppeteer/\n",
    "# !!! function could only be called with await !!!\n",
    "async def scrape(url_: str, selector_: str, page_function_ = \"(element) => element.outerHTML\",\n",
    "                 bypass_google_anti_scrape_algorithm_ = False, log_ = True):\n",
    "    if log_ : print(\"-------------------------Scrape Log Begin--------------------------\", \"\\n\")\n",
    "    #create random user agent so YouTube's algorithm gets pypassed\n",
    "    ua = UserAgent()\n",
    "    agent = ua.random\n",
    "    \n",
    "    # create browser, incognito context and page\n",
    "    browser = await launch()\n",
    "    context = await browser.createIncognitoBrowserContext()\n",
    "    page = await context.newPage()\n",
    "    if log_ : print(\"Browser, Incognito Context and Page created\")\n",
    "    \n",
    "    # set user agent\n",
    "    await page.setUserAgent(agent)\n",
    "    if log_ : print(\"User Agent:\", agent)\n",
    "    \n",
    "    # open url\n",
    "    await page.goto(url_)\n",
    "    if log_ : print(\"Url opened:\", url_)\n",
    "        \n",
    "    if bypass_google_anti_scrape_algorithm_:\n",
    "        await page.waitForSelector(\"h1.title\")\n",
    "        await page.click(\"h1.title\")\n",
    "        \n",
    "        # multiple scroll to page end to get more comments\n",
    "        for i in range(12):\n",
    "            time.sleep(5)\n",
    "            await page.keyboard.press(\"End\")\n",
    "    \n",
    "    # wait until page gets loaded\n",
    "    await page.waitForSelector(selector_)\n",
    "    if log_ : print(\"Selector loaded:\", selector_)\n",
    "        \n",
    "    await page.click(selector_)\n",
    "        \n",
    "    if bypass_google_anti_scrape_algorithm_:\n",
    "        time.sleep(3)\n",
    "        \n",
    "    await page.click(selector_)\n",
    "    \n",
    "    # get element from query selector and relating function\n",
    "    request_result = await page.querySelectorEval(selector_, page_function_)\n",
    "    if log_ : print(\"Request finished\")\n",
    "\n",
    "    # close browser\n",
    "    await browser.close()\n",
    "    if log_ : print(\"Browser closed\", \"\\n\")\n",
    "    if log_ : print(\"-------------------------Scrape Log End----------------------------\", \"\\n\")\n",
    "    \n",
    "    return request_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test the \"scrape\" method: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get YouTube Video Title\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\"\n",
    "query_selector = \"h1.title\"\n",
    "function = \"(element) => element.firstChild.innerHTML\"\n",
    "\n",
    "#title = await scrape(url, query_selector, function)                    \n",
    "#print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play around with BeautifulSoup for html parsing: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get comments and their authors as html\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\"\n",
    "query_selector = \"ytd-comments\"\n",
    "function = \"(element) => element.outerHTML\"\n",
    "\n",
    "#html = await scrape(url, query_selector, function, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parse html and assign them\n",
    "\n",
    "def _parse_comments_with_corresponding_authors(html_, log_ = True):\n",
    "    soup = BeautifulSoup(html_, features=\"html.parser\")\n",
    "\n",
    "    # get authors of comments and clear html data\n",
    "    authors = [item.text.strip() for item in soup.select(\"a[id=author-text] > span\")]\n",
    "\n",
    "    # get comments and clear html data\n",
    "    comments = [\n",
    "        item.text.strip().replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").replace(\"\\\"\", \"'\") \n",
    "        for item in soup.select(\"yt-formatted-string[id=content-text]\")\n",
    "    ]\n",
    "    #print(comments)\n",
    "\n",
    "    likes = [\n",
    "        item.text.strip().replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").replace(\".\",\"\").replace(\",\",\"\")\n",
    "        for item in soup.select(\"span[id=vote-count-middle]\")\n",
    "    ]\n",
    "    \n",
    "    #<span id=\"vote-count-middle\" class=\"style-scope ytd-comment-action-buttons-renderer\" aria-label=\"2&nbsp;&quot;Mag ich&quot;-Bewertungen\">\n",
    "    #print(likes)\n",
    "    comments_with_authors_and_likes = list(zip(authors, comments, likes))\n",
    "\n",
    "    \n",
    "    if log_:\n",
    "        print(\"Finished parsing\")\n",
    "        #for author, comment, likes in comments_with_authors_and_likes:\n",
    "        #    print(author, \"wrote:\\n -\" + comment + \" with \"+likes+\" likes\")\n",
    "    \n",
    "    return comments_with_authors_and_likes\n",
    "\n",
    "# Let's try it:\n",
    "#_parse_comments_with_corresponding_authors(html, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build parser methods: ##\n",
    "\n",
    "The first method we define will scrape video metadata like \n",
    "the name, likes, dislikes, date and so on... <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape and parse video metadata\n",
    "\n",
    "# returns metadata as dict\n",
    "# function is asynchronous and therefore it has to be awaited\n",
    "async def _scrape_and_parse_video_meta_data(url: str, log_ = True):\n",
    "    if \"youtube.com\" in url:\n",
    "        \n",
    "        trials = 1\n",
    "        max_trials = 5\n",
    "        \n",
    "        while trials < max_trials:\n",
    "            try:\n",
    "                html = await scrape(\n",
    "                    url, \n",
    "                    \"div#info-contents\",\n",
    "                    \"(element) => element.outerHTML\", \n",
    "                    bypass_google_anti_scrape_algorithm_ = (trials > 2),\n",
    "                    log_ = log_\n",
    "                )\n",
    "                \n",
    "                trials = 100000\n",
    "            except Exception as e:\n",
    "                print('WARNING! : Metadata Scraping trial',trials,'failed for url=\"',url,'\"!')\n",
    "                print('ERROR:\\n',e)\n",
    "                trials = trials + 1\n",
    "                \n",
    "                \n",
    "        if trials == max_trials : \n",
    "            raise Exception(\"Meta-Data scraping trials all failed!! :(\")\n",
    "       \n",
    "        \n",
    "        soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "        title = soup.find(\"h1\", {\"class\": \"title\"}).find(\"yt-formatted-string\").text\n",
    "        primary_info = soup.find_all(\"yt-formatted-string\", {\"class\": \"ytd-video-primary-info-renderer\"})\n",
    "        \n",
    "        date = (primary_info[len(primary_info) - 1].text)\n",
    "    \n",
    "        hashtags = [ tag.text.strip() for tag in primary_info[0].find_all(\"a\") if tag != None]\n",
    "        \n",
    "        likes = soup.select(\"yt-formatted-string[id=text]\")[0].text.replace(\".\", \"\").replace(\",\",\"\").replace(\"\\xa0Mio\",\"0\"*6)\n",
    "        dislikes = soup.select(\"yt-formatted-string[id=text]\")[1].text.replace(\".\", \"\").replace(\",\",\"\").replace(\"\\xa0Mio\",\"0\"*6)\n",
    "        \n",
    "        return {\"title\": title, \"date\": date, \"hashtags\": hashtags, \"likes\": likes, \"dislikes\": dislikes}\n",
    "    else:\n",
    "        print(\"Wrong url format given!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to gather the comments for a specific video! <br>\n",
    "The following method does exactly that.\n",
    "It takes a video url and runs the scrape method internally just like the previous method... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape and parse comments with authors\n",
    "\n",
    "# returns list of tuples [(Author, Comment), (...), ...]\n",
    "# function is asynchronous and therefore it has to be awaited\n",
    "async def _scrape_and_parse_youtube_comments(url: str, log_ = True):\n",
    "    \n",
    "    if \"youtube.com\" in url:\n",
    "        trials = 1\n",
    "        max_trials = 5\n",
    "        \n",
    "        while trials < max_trials:\n",
    "            try:\n",
    "                html = await scrape(url, \"ytd-comments\", \"(element) => element.outerHTML\", bypass_google_anti_scrape_algorithm_ = True, log_ = log_)\n",
    "        \n",
    "                return _parse_comments_with_corresponding_authors(html, log_ = log_)\n",
    "        \n",
    "            except Exception as e:\n",
    "                print('WARNING! : Comment scraping trial',trials,'failed for url=\"',url,'\"!')\n",
    "                print('ERROR:\\n',e)\n",
    "                trials = trials + 1\n",
    "\n",
    "\n",
    "        if trials == max_trials : \n",
    "            raise Exception(\"Comment scraping trials all failed!! :(\")\n",
    "    else:\n",
    "        print(\"Wrong url format given!\")\n",
    "            \n",
    "\n",
    "# Let's test it:      \n",
    "#await _scrape_and_parse_youtube_comments(\"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\", log_ = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to store the data into the database! <br>\n",
    "This is done via the following two methods.\n",
    "The first one handles storing video metadata and the second one\n",
    "stores the video comments. <br>\n",
    "As you can see we user cypher queries which will be send to the neo4j database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping transaction : \n",
    "\n",
    "def _store_video_metadata(tx, httpUrl_, metadata_): # \"tx\" is a neo4j transaction...\n",
    "     \n",
    "    merges = '\\n'.join([\n",
    "        \"MERGE(t\"+str(i)+':Tag{name:\"'+str(t)+'\"})\\n' + \n",
    "        \"MERGE(t\"+str(i)+\")-[:REFERENCES]->(v)\" \n",
    "        for i, t in enumerate(metadata_['hashtags'])\n",
    "    ])  \n",
    "    result = tx.run(\n",
    "        \"CREATE (v:Video) \"\n",
    "        \"SET v = {title: $title, date: $date, likes: $likes, dislikes:$dislikes, url: $url}\\n\"+merges+\"\\n\"\n",
    "        \"RETURN v.title + ', from node ' + id(v)\", \n",
    "        title=metadata_[\"title\"], \n",
    "        date=metadata_[\"date\"], \n",
    "        likes=metadata_[\"likes\"], \n",
    "        dislikes=metadata_[\"dislikes\"],\n",
    "        url=httpUrl_\n",
    "    )\n",
    "    print('Video Metadata\"', metadata_,'\" sent to database...')\n",
    "    return result\n",
    "\n",
    "def _store_video_comments(tx, httpUrl_, data):\n",
    "    comments_with_authors_, ratio = data\n",
    "    author_result = []\n",
    "    comment_result = []\n",
    "    for author, comment, likes in comments_with_authors_:\n",
    "        author_result.append(tx.run(\n",
    "            \"MERGE (a:Author{name: $name})\"\n",
    "            \"RETURN a.name + ', created as Author with id ' + id(a)\", \n",
    "            name=author\n",
    "        )) \n",
    "        comment_result.append(tx.run(\n",
    "            \"\"\"\n",
    "                MATCH (v:Video), (a:Author)\n",
    "                WHERE v.url = \"%s\" AND a.name = \"%s\"\n",
    "                CREATE (a) - [r:%s { text: \"%s\", likes: %s, score: %s }] -> (v)\n",
    "                RETURN v.title, type(r), r.text, a.name\n",
    "            \"\"\" % ( httpUrl_, author, \"COMMENTED\", comment, likes, ratio*int(likes))))\n",
    "        \n",
    "    print('Comments sent to database...')\n",
    "    return zip(author_result, comment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neo4j for data storage : ##\n",
    "\n",
    "Now that we have everything set up we will proceed to \n",
    "go through a list of video urls and scrape them for their data! :)<br>\n",
    "All we need to do is call the previously defined methods for scraping and storing urls..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri, user, password = 'bolt://localhost:7687', 'neo4j', 'neo4j_'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================================\n",
      "| SCRAPING VIDEO : https://www.youtube.com/watch?v=Ul0ZgDoamco\n",
      "======================================================================================\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.137 Safari/4E423F\n",
      "Url opened: https://www.youtube.com/watch?v=Ul0ZgDoamco\n",
      "Selector loaded: div#info-contents\n",
      "Request finished\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "Video Metadata\" {'title': 'Sentiment Analysis Python - 2 -  Creating Project and Installing Python (NLP)', 'date': '04.03.2020', 'hashtags': ['#python', '#nltk', '#nlp'], 'likes': '49', 'dislikes': '3'} \" sent to database...\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; AS; rv:11.0) like Gecko\n",
      "Url opened: https://www.youtube.com/watch?v=Ul0ZgDoamco\n",
      "WARNING! : Comment scraping trial 1 failed for url=\" https://www.youtube.com/watch?v=Ul0ZgDoamco \"!\n",
      "ERROR:\n",
      " Waiting for selector \"h1.title\" failed: timeout 30000ms exceeds.\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1623.0 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=Ul0ZgDoamco\n",
      "Selector loaded: ytd-comments\n",
      "Request finished\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "Finished parsing\n",
      "Comments sent to database...\n",
      "\n",
      "======================================================================================\n",
      "| SCRAPING VIDEO : https://www.youtube.com/watch?v=t-h1BhO4V_U\n",
      "======================================================================================\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2117.157 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=t-h1BhO4V_U\n",
      "Selector loaded: div#info-contents\n",
      "Request finished\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "Video Metadata\" {'title': 'Sentiment Analysis Python - 7 -  Emotions in a Graph using Matplotlib (NLP)', 'date': '04.03.2020', 'hashtags': ['#python', '#nltk', '#nlp'], 'likes': '77', 'dislikes': '0'} \" sent to database...\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=t-h1BhO4V_U\n"
     ]
    }
   ],
   "source": [
    "httpUrls = [\n",
    "    # please do not shuffle first ten videos --> they are needed for data visualization\n",
    "    \"https://www.youtube.com/watch?v=dyN_WtjdfpA\",\n",
    "    \"https://www.youtube.com/watch?v=Ul0ZgDoamco\",\n",
    "    \"https://www.youtube.com/watch?v=lcgqP8g6i84\",\n",
    "    \"https://www.youtube.com/watch?v=KrEhmADXTr8\",\n",
    "    \"https://www.youtube.com/watch?v=cV_uKzKZsHs\",\n",
    "    \"https://www.youtube.com/watch?v=7plsgTxJn88\",\n",
    "    \"https://www.youtube.com/watch?v=t-h1BhO4V_U\",\n",
    "    \"https://www.youtube.com/watch?v=yufg9PdxrWs\",\n",
    "    \"https://www.youtube.com/watch?v=tQ_nVSxjn_s\",\n",
    "    \"https://www.youtube.com/watch?v=HtYweBOCp7A\",\n",
    "\n",
    "    \n",
    "    \n",
    "    \"https://www.youtube.com/watch?v=N_7eYLpynOs\",\n",
    "    \"https://www.youtube.com/watch?v=tpiyEe_CqB4\",\n",
    "    \"https://www.youtube.com/watch?v=HbaIfdEAkII\",\n",
    "    \"https://www.youtube.com/watch?v=VoUD5MDfCEM\",\n",
    "    \"https://www.youtube.com/watch?v=_pLTNTyD6nw\",\n",
    "    \"https://www.youtube.com/watch?v=CAyWN9ba9J8\",\n",
    "    \"https://www.youtube.com/watch?v=N274EurzpAA\",\n",
    "    \"https://www.youtube.com/watch?v=L0H6xYwMQnk\",\n",
    "    \"https://www.youtube.com/watch?v=rB83DpBJQsE\",\n",
    "    \"https://www.youtube.com/watch?v=_DPRt3AcUEY\",\n",
    "    \"https://www.youtube.com/watch?v=Sw9r8CL98N0\",\n",
    "    \"https://www.youtube.com/watch?v=dyN_WtjdfpA\",\n",
    "    \"https://www.youtube.com/watch?v=Ul0ZgDoamco\",\n",
    "    \"https://www.youtube.com/watch?v=lcgqP8g6i84\",\n",
    "    \"https://www.youtube.com/watch?v=KrEhmADXTr8\",\n",
    "    \"https://www.youtube.com/watch?v=Iot0eF6EoNA\",\n",
    "    \"https://www.youtube.com/watch?v=wx9Jv5uxfac\",\n",
    "    \"https://www.youtube.com/watch?v=cV_uKzKZsHs\",\n",
    "    \"https://www.youtube.com/watch?v=7plsgTxJn88\",\n",
    "    \"https://www.youtube.com/watch?v=t-h1BhO4V_U\",\n",
    "    \"https://www.youtube.com/watch?v=yufg9PdxrWs\",\n",
    "    \"https://www.youtube.com/watch?v=tQ_nVSxjn_s\",\n",
    "    \"https://www.youtube.com/watch?v=HtYweBOCp7A\",\n",
    "    \"https://www.youtube.com/watch?v=L2c9q6zVp-I\",\n",
    "    \"https://www.youtube.com/watch?v=hp_6u_MjjJ0\",\n",
    "    \"https://www.youtube.com/watch?v=i9pNYW1Pg9A\",\n",
    "    \"https://www.youtube.com/watch?v=MAlSjtxy5ak\",\n",
    "    \"https://www.youtube.com/watch?v=pKO9UjSeLew\",\n",
    "    \"https://www.youtube.com/watch?v=OTfp2_SwxHk\",\n",
    "    \"https://www.youtube.com/watch?v=SzJ46YA_RaA\",\n",
    "    \"https://www.youtube.com/watch?v=OmJ-4B-mS-Y\",\n",
    "    \"https://www.youtube.com/watch?v=2a-CVGbUUjQ\",\n",
    "    \"https://www.youtube.com/watch?v=cfDAdI2r-fw\",\n",
    "    \"https://www.youtube.com/watch?v=MXJ-zpJeY3E\",\n",
    "    \"https://www.youtube.com/watch?v=uTxRF5ag27A\",\n",
    "    \"https://www.youtube.com/watch?v=IT__Nrr3PNI\",\n",
    "    \"https://www.youtube.com/watch?v=C1vW9iSpLLk\",\n",
    "    \"https://www.youtube.com/watch?v=kTXTPe3wahc\",\n",
    "    \"https://www.youtube.com/watch?v=YbJOTdZBX1g\",\n",
    "    \"https://www.youtube.com/watch?v=XUyoJ79P2SQ\",\n",
    "    \"https://www.youtube.com/watch?v=CZIt20emgLY\",\n",
    "    \"https://www.youtube.com/watch?v=XmIIgE7eSak\",\n",
    "    \"https://www.youtube.com/watch?v=nIR2GR254S8\",\n",
    "    \"https://www.youtube.com/watch?v=MO7erSNZszI\",\n",
    "    \"https://www.youtube.com/watch?v=ASFSXNQKPDI\",\n",
    "    \"https://www.youtube.com/watch?v=qjJcomKo_O8\",\n",
    "    \"https://www.youtube.com/watch?v=ec65fZXcWv4\",\n",
    "    \"https://www.youtube.com/watch?v=gFEjCuae8RA\",\n",
    "    \"https://www.youtube.com/watch?v=i4XvlM_j3A0\",\n",
    "    \"https://www.youtube.com/watch?v=1zZZjaYl4AA\",\n",
    "    \"https://www.youtube.com/watch?v=qJW0LUE1RUA\"\n",
    "    \"https://www.youtube.com/watch?v=LKOl8LoXPSY\",\n",
    "    \"https://www.youtube.com/watch?v=RZEb_utxH4s\",\n",
    "    \"https://www.youtube.com/watch?v=1vWTJzJx0i4\",\n",
    "    \"https://www.youtube.com/watch?v=DxZVfwudvC8\",\n",
    "    \"https://www.youtube.com/watch?v=BiqDZlAZygU\",\n",
    "    \"https://www.youtube.com/watch?v=ulsQpeEUxx0\",\n",
    "    \"https://www.youtube.com/watch?v=Iyt8rYpUQsg\",\n",
    "    \"https://www.youtube.com/watch?v=vgxCnXdHi68\",\n",
    "    \"https://www.youtube.com/watch?v=QQ9gs-5lRKc\",\n",
    "    \"https://www.youtube.com/watch?v=0jspaMLxBig\",\n",
    "    \"https://www.youtube.com/watch?v=OiPVgAhRE6E\",\n",
    "    \"https://www.youtube.com/watch?v=ggyVXHU0_ms\",\n",
    "    \"https://www.youtube.com/watch?v=X6p5AZp7r_Q\",\n",
    "    \"https://www.youtube.com/watch?v=DqgDqFrBnlQ\",\n",
    "    \"https://www.youtube.com/watch?v=w3ugHP-yZXw\",\n",
    "    \"https://www.youtube.com/watch?v=2lAe1cqCOXo\",\n",
    "    \"https://www.youtube.com/watch?v=3o3imqMkOYs\",\n",
    "    \"https://www.youtube.com/watch?v=u1V-7UiXNZg\",\n",
    "    \"https://www.youtube.com/watch?v=FJ3oHpup-pk\",\n",
    "    \"https://www.youtube.com/watch?v=M0O7lLe4SmA\",\n",
    "    \"https://www.youtube.com/watch?v=IvxRsDpXPGo\",\n",
    "    \"https://www.youtube.com/watch?v=kffacxfA7G4\",\n",
    "    \"https://www.youtube.com/watch?v=YbJOTdZBX1g\",\n",
    "    \"https://www.youtube.com/watch?v=o0vD7vqP1M8\",\n",
    "    \"https://www.youtube.com/watch?v=067wXeqPCQU\",\n",
    "    \"https://www.youtube.com/watch?v=QwZT7T-TXT0\",\n",
    "    \"https://www.youtube.com/watch?v=rRAc29qoywA\",\n",
    "    \"https://www.youtube.com/watch?v=Iot0eF6EoNA\",\n",
    "    \"https://www.youtube.com/watch?v=kfVsfOSbJY0\",\n",
    "    \"https://www.youtube.com/watch?v=hSlb1ezRqfA\",\n",
    "    \"https://www.youtube.com/watch?v=4gSOMba1UdM\",\n",
    "    \"https://www.youtube.com/watch?v=b6Yki9Pr-7k\",\n",
    "    \"https://www.youtube.com/watch?v=OSjBTegtHqE\",\n",
    "    \"https://www.youtube.com/watch?v=vfc42Pb5RA8\",\n",
    "    \"https://www.youtube.com/watch?v=NoQil9EWjxo\",\n",
    "    \"https://www.youtube.com/watch?v=AFCGjW6Bwjs\",\n",
    "    \"https://www.youtube.com/watch?v=WFcGOHEAypM\",\n",
    "    \"https://www.youtube.com/watch?v=75U0YdxIi8M\",\n",
    "    \"https://www.youtube.com/watch?v=FlsCjmMhFmw\",\n",
    "    \"https://www.youtube.com/watch?v=2lAe1cqCOXo\",\n",
    "    \"https://www.youtube.com/watch?v=EeF3UTkCoxY\",\n",
    "    \"https://www.youtube.com/watch?v=1RwQlgS4zQI\",    \n",
    "    \"https://www.youtube.com/watch?v=ulsQpeEUxx0\",\n",
    "    \"https://www.youtube.com/watch?v=3zhCbaVBEjg\",\n",
    "    \"https://www.youtube.com/watch?v=cbqft9HbmDE\",\n",
    "    \"https://www.youtube.com/watch?v=eIrMbAQSU34\"\n",
    "]\n",
    "\n",
    "import random \n",
    "random.Random(66642999).shuffle(httpUrls) # First we shuffle the urls so that the data is randomly stored!\n",
    "\n",
    "httpUrls = list(set(httpUrls)) # removing duplicate videos if some exist...\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# resetting database\n",
    "\n",
    "with driver.session() as session:\n",
    "    def _q(query) : return session.run(query)\n",
    "    #---------------------------------------\n",
    "\n",
    "    _q(\"MATCH (n) DETACH DELETE n\") # remove all graphs and nodes! BE CAREFUL!\n",
    "\n",
    "    #---------------------------------------\n",
    "driver.close()\n",
    "\n",
    "# activate logging\n",
    "log_process = True\n",
    "\n",
    "\n",
    "with driver.session() as session:\n",
    "    for url in httpUrls :\n",
    "        # run await outside of transaction because asynchronous transactions for Neo4j are not yet available for Python\n",
    "        print(\"\\n======================================================================================\")\n",
    "        print(\"| SCRAPING VIDEO : \"+url)\n",
    "        print(\"======================================================================================\")\n",
    "        try:\n",
    "            metadata = await _scrape_and_parse_video_meta_data(url, log_ = log_process)\n",
    "            result = session.write_transaction(_store_video_metadata, url, metadata)\n",
    "            #print(result)\n",
    "\n",
    "            comments_with_authors = await _scrape_and_parse_youtube_comments(url, log_ = log_process)\n",
    "            if len(comments_with_authors) == 0 :\n",
    "                print(\"Video without comments found! This might be wrong!\")\n",
    "                print(\"Let's try again...\")\n",
    "                comments_with_authors = await _scrape_and_parse_youtube_comments(url, log_ = log_process)\n",
    "\n",
    "            ratio = (int(metadata['likes']))/(int(metadata['likes'])+int(metadata['dislikes']))*2 - 1 \n",
    "\n",
    "            data = (comments_with_authors, ratio)\n",
    "            result = session.write_transaction(_store_video_comments, url, data)\n",
    "            #print(result)\n",
    "        except :\n",
    "            print('Failed loading url:', url)\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
