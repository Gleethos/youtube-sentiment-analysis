{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Youtube - Scaper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyppeteer import launch\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# for extended documentation visit --> https://miyakogi.github.io/pyppeteer/\n",
    "# !!! function could only be called with await !!!\n",
    "async def scrape(url_: str, selector_: str, page_function_ = \"(element) => element.outerHTML\",\n",
    "                 bypass_google_anti_scrape_algorithm_ = False, log_ = True):\n",
    "    if log_ : print(\"-------------------------Scrape Log Begin--------------------------\", \"\\n\")\n",
    "    #create random user agent so YouTube's algorithm gets pypassed\n",
    "    ua = UserAgent()\n",
    "    agent = ua.random\n",
    "    \n",
    "    # create browser, incognito context and page\n",
    "    browser = await launch()\n",
    "    context = await browser.createIncognitoBrowserContext()\n",
    "    page = await context.newPage()\n",
    "    if log_ : print(\"Browser, Incognito Context and Page created\")\n",
    "    \n",
    "    # set user agent\n",
    "    await page.setUserAgent(agent)\n",
    "    if log_ : print(\"User Agent:\", agent)\n",
    "    \n",
    "    # open url\n",
    "    await page.goto(url_)\n",
    "    if log_ : print(\"Url opened:\", url_)\n",
    "        \n",
    "    if bypass_google_anti_scrape_algorithm_:\n",
    "        await page.waitForSelector(\"h1.title\")\n",
    "        await page.click(\"h1.title\")\n",
    "        time.sleep(5)\n",
    "        await page.keyboard.press(\"End\")\n",
    "    \n",
    "    # wait until page gets loaded\n",
    "    await page.waitForSelector(selector_)\n",
    "    if log_ : print(\"Selector loaded:\", selector_)\n",
    "        \n",
    "    await page.click(selector_)\n",
    "        \n",
    "    if bypass_google_anti_scrape_algorithm_:\n",
    "        time.sleep(3)\n",
    "        \n",
    "    await page.click(selector_)\n",
    "    \n",
    "    # get element from query selector and relating function\n",
    "    request_result = await page.querySelectorEval(selector_, page_function_)\n",
    "    if log_ : print(\"Request finished\")\n",
    "\n",
    "    # close browser\n",
    "    await browser.close()\n",
    "    if log_ : print(\"Browser closed\", \"\\n\")\n",
    "    if log_ : print(\"-------------------------Scrape Log End----------------------------\", \"\\n\")\n",
    "    \n",
    "    return request_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test the \"scrape\" method: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.137 Safari/4E423F\n",
      "Url opened: https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\n",
      "Selector loaded: h1.title\n",
      "Request finished\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "Sentiment Analysis Python - 1 -  Introduction to Emotion Analysis  (NLP)\n"
     ]
    }
   ],
   "source": [
    "# get YouTube Video Title\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\"\n",
    "query_selector = \"h1.title\"\n",
    "function = \"(element) => element.firstChild.innerHTML\"\n",
    "\n",
    "title = await scrape(url, query_selector, function)\n",
    "                      \n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play around with BeautifulSoup for html parsing: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (X11; Ubuntu; Linux i686 on x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2820.59 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\n",
      "Selector loaded: ytd-comments\n",
      "Request finished\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get comments and their authors as html\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\"\n",
    "query_selector = \"ytd-comments\"\n",
    "function = \"(element) => element.outerHTML\"\n",
    "\n",
    "html = await scrape(url, query_selector, function, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('morthim', \"i'm really impressed by your emotions dict.\"),\n",
       " ('vaishnavi kulkarni',\n",
       "  'your space invader game tutorial is amazing! Hope to see more videos on python during Quarantine!!'),\n",
       " ('IndieSpaceAstronaut', 'Love the hashtags lol'),\n",
       " ('reda bekka', 'PLease can u make a course of mouvment detections'),\n",
       " ('Dr Scary', 'Our Boiii Is Back Lol :)'),\n",
       " ('Usama Iftikhar Butt', 'please upload 2 3 videos weekly'),\n",
       " ('Usama Iftikhar Butt', 'great series'),\n",
       " ('Alma Jose',\n",
       "  'can someone please give me code for finding different emotions like sad happy angry and neutral from lyrics of song.'),\n",
       " ('Aditya Pai Thon', 'Boii is bacccc'),\n",
       " ('Hasna Bouazza', 'could we have the code .py')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse html and assign them\n",
    "\n",
    "def _parse_comments_with_corresponding_authors(html_, log_ = True):\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # get authors of comments and clear html data\n",
    "    authors = [item.text.strip() for item in soup.select(\"a[id=author-text] > span\")]\n",
    "\n",
    "    # get comments and clear html data\n",
    "    comments = [item.text.strip().replace(\"\\n\", \" \") for item in soup.select(\"yt-formatted-string[id=content-text]\")]\n",
    "\n",
    "    comments_with_authors = list(zip(authors, comments))\n",
    "    \n",
    "    if log_:\n",
    "        print(\"Finished parsing\")\n",
    "        #for author, comment in comments_with_authors:\n",
    "        #    print(author, \"wrote:\\n -\" + comment)\n",
    "    \n",
    "    return comments_with_authors\n",
    "\n",
    "_parse_comments_with_corresponding_authors(html, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build parser methods: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape and parse video metadata\n",
    "\n",
    "# returns metadata as dict\n",
    "# function is asynchronous and therefore it has to be awaited\n",
    "async def _scrape_and_parse_video_meta_data(url: str, log_ = True):\n",
    "    if \"youtube.com\" in url:\n",
    "        \n",
    "        trials = 1\n",
    "        \n",
    "        while trials <= 2:\n",
    "            try:\n",
    "                html = await scrape(\n",
    "                    url, \n",
    "                    \"div#info-contents\",\n",
    "                    \"(element) => element.outerHTML\", \n",
    "                    bypass_google_anti_scrape_algorithm_ = (trials != 2),\n",
    "                    log_ = log_\n",
    "                )\n",
    "                \n",
    "                trials = 100000\n",
    "            except:\n",
    "                print('WARNING! : Metadata Scraping trial',trials,'failed for url=\"',url,'\"!') \n",
    "                trials += 1\n",
    "                \n",
    "        \n",
    "        soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "        title = soup.find(\"h1\", {\"class\": \"title\"}).find(\"yt-formatted-string\").text\n",
    "        primary_info = soup.find_all(\"yt-formatted-string\", {\"class\": \"ytd-video-primary-info-renderer\"})\n",
    "        \n",
    "        date = (primary_info[len(primary_info) - 1].text)\n",
    "    \n",
    "        hashtags = [ tag.text.strip() for tag in primary_info[0].find_all(\"a\") if tag != None]\n",
    "        \n",
    "        likes = soup.select(\"yt-formatted-string[id=text]\")[0].text\n",
    "        dislikes = soup.select(\"yt-formatted-string[id=text]\")[1].text\n",
    "        \n",
    "        return {\"title\": title, \"date\": date, \"hashtags\": hashtags, \"likes\": likes, \"dislikes\": dislikes}\n",
    "    else:\n",
    "        print(\"Wrong url format given!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('morthim', \"i'm really impressed by your emotions dict.\"),\n",
       " ('vaishnavi kulkarni',\n",
       "  'your space invader game tutorial is amazing! Hope to see more videos on python during Quarantine!!'),\n",
       " ('IndieSpaceAstronaut', 'Love the hashtags lol'),\n",
       " ('reda bekka', 'PLease can u make a course of mouvment detections'),\n",
       " ('Dr Scary', 'Our Boiii Is Back Lol :)'),\n",
       " ('Usama Iftikhar Butt', 'please upload 2 3 videos weekly'),\n",
       " ('Usama Iftikhar Butt', 'great series'),\n",
       " ('Alma Jose',\n",
       "  'can someone please give me code for finding different emotions like sad happy angry and neutral from lyrics of song.'),\n",
       " ('Aditya Pai Thon', 'Boii is bacccc'),\n",
       " ('Hasna Bouazza', 'could we have the code .py')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape and parse comments with authors\n",
    "\n",
    "# returns list of tuples [(Author, Comment), (...), ...]\n",
    "# function is asynchronous and therefore it has to be awaited\n",
    "async def _scrape_and_parse_youtube_comments(url: str, log_ = True):\n",
    "    if \"youtube.com\" in url:\n",
    "        html = await scrape(url, \"ytd-comments\", \"(element) => element.outerHTML\", bypass_google_anti_scrape_algorithm_ = True, log_ = log_)\n",
    "        \n",
    "        return _parse_comments_with_corresponding_authors(html, log_ = log_)\n",
    "    else:\n",
    "        print(\"Wrong url format given!\")\n",
    "        \n",
    "await _scrape_and_parse_youtube_comments(\"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\", log_ = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scraping transaction : \n",
    "\n",
    "def _scrape_and_store_video_metadata(tx, httpUrl_, metadata_): # \"tx\" is a neo4j transaction...\n",
    "    print(metadata)\n",
    "    result = tx.run(\"CREATE (v:Video) \"\n",
    "                    \"SET v = {title: $title, date: $date, likes: $likes, dislikes:$dislikes, url: $url}\"\n",
    "                    \"RETURN v.title + ', from node ' + id(v)\", title=metadata_[\"title\"], date=metadata[\"date\"], likes=metadata[\"likes\"], dislikes=metadata[\"dislikes\"],\n",
    "                                                                url=httpUrl_)\n",
    "    print('Video Metadata\"', metadata_,'\" sent to database...')\n",
    "    return result\n",
    "\n",
    "def _scrape_and_store_video_comments(tx, httpUrl_, comments_with_authors_):\n",
    "    author_result = []\n",
    "    comment_result = []\n",
    "    for author, comment in comments_with_authors_:\n",
    "        author_result.append(tx.run(\"CREATE (a:Author)\"\n",
    "                  \"SET a = {name: $name}\"\n",
    "                  \"RETURN a.name + ', created as Author with id ' + id(a)\", name=author))\n",
    "        \n",
    "        comment_result.append(tx.run(\n",
    "            \"\"\"\n",
    "                MATCH (v:Video), (a:Author)\n",
    "                WHERE v.url = '%s' AND a.name = '%s'\n",
    "                CREATE (a) - [r:%s { text: \"%s\" }] -> (v)\n",
    "                RETURN v.title, type(r), r.text, a.name\n",
    "            \"\"\" % ( httpUrl_, author, \"COMMENTED\", comment)))\n",
    "    print('Comments send to database...')\n",
    "    return zip(author_result, comment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neo4j for data storage : ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri, user, password = 'bolt://localhost:7687', 'neo4j', 'neo4j_'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.3319.102 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\n",
      "Selector loaded: ytd-comments\n",
      "Request finished\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "Finished parsing\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\n",
      "Selector loaded: div#info-contents\n",
      "Request finished\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "{'title': 'Sentiment Analysis Python - 1 -  Introduction to Emotion Analysis  (NLP)', 'date': '04.03.2020', 'hashtags': ['#donaldtrump', '#python', '#nltk'], 'likes': '181', 'dislikes': '3'}\n",
      "Video Metadata\" {'title': 'Sentiment Analysis Python - 1 -  Introduction to Emotion Analysis  (NLP)', 'date': '04.03.2020', 'hashtags': ['#donaldtrump', '#python', '#nltk'], 'likes': '181', 'dislikes': '3'} \" sent to database...\n",
      "Comments send to database...\n"
     ]
    }
   ],
   "source": [
    "httpUrls = [\n",
    "    \"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\"\n",
    "]\n",
    "\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# resetting database\n",
    "\n",
    "with driver.session() as session:\n",
    "    def _q(query) : return session.run(query)\n",
    "    #---------------------------------------\n",
    "\n",
    "    _q(\"MATCH (n) DETACH DELETE n\") # remove all graphs and nodes! BE CAREFUL!\n",
    "\n",
    "    #---------------------------------------\n",
    "driver.close()\n",
    "\n",
    "with driver.session() as session:\n",
    "    for url in httpUrls :\n",
    "        trials = 1\n",
    "        while trials <= 3 :\n",
    "            try :\n",
    "                # run await outside of transaction because asynchronous transactions for Neo4j are not yet available for Python\n",
    "                comments_with_authors = await _scrape_and_parse_youtube_comments(url, log_ = True)\n",
    "                metadata = await _scrape_and_parse_video_meta_data(url, log_ = True)\n",
    "\n",
    "                result = session.write_transaction(_scrape_and_store_video_metadata, url, metadata)\n",
    "                #print(result)\n",
    "\n",
    "                result = session.write_transaction(_scrape_and_store_video_comments, url, comments_with_authors)\n",
    "                #print(result)\n",
    "                trials = 100000\n",
    "            except Exception as e:\n",
    "                print('WARNING! : Scraping trial',trials,'failed for url=\"',url,'\"!') \n",
    "                print(e)\n",
    "                trials += 1\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
