{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Youtube - Scaper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyppeteer import launch\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# for extended documentation visit --> https://miyakogi.github.io/pyppeteer/\n",
    "# !!! function could only be called with await !!!\n",
    "async def scrape(url_: str, selector_: str, page_function_ = \"(element) => element.outerHTML\",\n",
    "                 bypass_google_anti_scrape_algorithm_ = False, log_ = True):\n",
    "    if log_ : print(\"-------------------------Scrape Log Begin--------------------------\", \"\\n\")\n",
    "    #create random user agent so YouTube's algorithm gets pypassed\n",
    "    ua = UserAgent()\n",
    "    agent = ua.random\n",
    "    \n",
    "    # create browser, incognito context and page\n",
    "    browser = await launch()\n",
    "    context = await browser.createIncognitoBrowserContext()\n",
    "    page = await context.newPage()\n",
    "    if log_ : print(\"Browser, Incognito Context and Page created\")\n",
    "    \n",
    "    # set user agent\n",
    "    await page.setUserAgent(agent)\n",
    "    if log_ : print(\"User Agent:\", agent)\n",
    "    \n",
    "    # open url\n",
    "    await page.goto(url_)\n",
    "    if log_ : print(\"Url opened:\", url_)\n",
    "        \n",
    "    if bypass_google_anti_scrape_algorithm_:\n",
    "        await page.waitForSelector(\"h1.title\")\n",
    "        await page.click(\"h1.title\")\n",
    "        time.sleep(5)\n",
    "        await page.keyboard.press(\"End\")\n",
    "    \n",
    "    # wait until page gets loaded\n",
    "    await page.waitForSelector(selector_)\n",
    "    if log_ : print(\"Selector loaded:\", selector_)\n",
    "        \n",
    "    await page.click(selector_)\n",
    "        \n",
    "    if bypass_google_anti_scrape_algorithm_:\n",
    "        time.sleep(3)\n",
    "        \n",
    "    await page.click(selector_)\n",
    "    \n",
    "    # get element from query selector and relating function\n",
    "    request_result = await page.querySelectorEval(selector_, page_function_)\n",
    "    if log_ : print(\"Request finished\")\n",
    "\n",
    "    # close browser\n",
    "    await browser.close()\n",
    "    if log_ : print(\"Browser closed\", \"\\n\")\n",
    "    if log_ : print(\"-------------------------Scrape Log End----------------------------\", \"\\n\")\n",
    "    \n",
    "    return request_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test the \"scrape\" method: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get YouTube Video Title\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\"\n",
    "query_selector = \"h1.title\"\n",
    "function = \"(element) => element.firstChild.innerHTML\"\n",
    "\n",
    "#title = await scrape(url, query_selector, function)                    \n",
    "#print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play around with BeautifulSoup for html parsing: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get comments and their authors as html\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\"\n",
    "query_selector = \"ytd-comments\"\n",
    "function = \"(element) => element.outerHTML\"\n",
    "\n",
    "#html = await scrape(url, query_selector, function, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parse html and assign them\n",
    "\n",
    "def _parse_comments_with_corresponding_authors(html_, log_ = True):\n",
    "    soup = BeautifulSoup(html_, features=\"html.parser\")\n",
    "\n",
    "    # get authors of comments and clear html data\n",
    "    authors = [item.text.strip() for item in soup.select(\"a[id=author-text] > span\")]\n",
    "\n",
    "    # get comments and clear html data\n",
    "    comments = [\n",
    "        item.text.strip().replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").replace(\"\\\"\", \"'\") \n",
    "        for item in soup.select(\"yt-formatted-string[id=content-text]\")\n",
    "    ]\n",
    "    print(comments)\n",
    "\n",
    "    likes = [\n",
    "        item.text.strip().replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \") \n",
    "        for item in soup.select(\"span[id=vote-count-middle]\")\n",
    "    ]\n",
    "    \n",
    "    #<span id=\"vote-count-middle\" class=\"style-scope ytd-comment-action-buttons-renderer\" aria-label=\"2&nbsp;&quot;Mag ich&quot;-Bewertungen\">\n",
    "    #print(likes)\n",
    "    comments_with_authors_and_likes = list(zip(authors, comments, likes))\n",
    "\n",
    "    \n",
    "    if log_:\n",
    "        print(\"Finished parsing\")\n",
    "        #for author, comment, likes in comments_with_authors_and_likes:\n",
    "        #    print(author, \"wrote:\\n -\" + comment + \" with \"+likes+\" likes\")\n",
    "    \n",
    "    return comments_with_authors_and_likes\n",
    "\n",
    "# Let's try it:\n",
    "#_parse_comments_with_corresponding_authors(html, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build parser methods: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape and parse video metadata\n",
    "\n",
    "# returns metadata as dict\n",
    "# function is asynchronous and therefore it has to be awaited\n",
    "async def _scrape_and_parse_video_meta_data(url: str, log_ = True):\n",
    "    if \"youtube.com\" in url:\n",
    "        \n",
    "        trials = 1\n",
    "        \n",
    "        while trials <= 2:\n",
    "            try:\n",
    "                html = await scrape(\n",
    "                    url, \n",
    "                    \"div#info-contents\",\n",
    "                    \"(element) => element.outerHTML\", \n",
    "                    bypass_google_anti_scrape_algorithm_ = (trials != 2),\n",
    "                    log_ = log_\n",
    "                )\n",
    "                \n",
    "                trials = 100000\n",
    "            except Exception as e:\n",
    "                print('WARNING! : Metadata Scraping trial',trials,'failed for url=\"',url,'\"!')\n",
    "                print('ERROR:\\n',e)\n",
    "                trials += 1\n",
    "                \n",
    "                \n",
    "        if trials == 3 : \n",
    "            raise Exception(\"Meta-Data scraping trials all failed!! :(\")\n",
    "       \n",
    "        \n",
    "        soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "        title = soup.find(\"h1\", {\"class\": \"title\"}).find(\"yt-formatted-string\").text\n",
    "        primary_info = soup.find_all(\"yt-formatted-string\", {\"class\": \"ytd-video-primary-info-renderer\"})\n",
    "        \n",
    "        date = (primary_info[len(primary_info) - 1].text)\n",
    "    \n",
    "        hashtags = [ tag.text.strip() for tag in primary_info[0].find_all(\"a\") if tag != None]\n",
    "        \n",
    "        likes = soup.select(\"yt-formatted-string[id=text]\")[0].text\n",
    "        dislikes = soup.select(\"yt-formatted-string[id=text]\")[1].text\n",
    "        \n",
    "        return {\"title\": title, \"date\": date, \"hashtags\": hashtags, \"likes\": likes, \"dislikes\": dislikes}\n",
    "    else:\n",
    "        print(\"Wrong url format given!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape and parse comments with authors\n",
    "\n",
    "# returns list of tuples [(Author, Comment), (...), ...]\n",
    "# function is asynchronous and therefore it has to be awaited\n",
    "async def _scrape_and_parse_youtube_comments(url: str, log_ = True):\n",
    "    \n",
    "    if \"youtube.com\" in url:\n",
    "        trials = 1\n",
    "        \n",
    "        while trials <= 2:\n",
    "            try:\n",
    "                html = await scrape(url, \"ytd-comments\", \"(element) => element.outerHTML\", bypass_google_anti_scrape_algorithm_ = True, log_ = log_)\n",
    "        \n",
    "                return _parse_comments_with_corresponding_authors(html, log_ = log_)\n",
    "        \n",
    "            except Exception as e:\n",
    "                print('WARNING! : Comment scraping trial',trials,'failed for url=\"',url,'\"!')\n",
    "                print('ERROR:\\n',e)\n",
    "                trials += 1\n",
    "\n",
    "\n",
    "        if trials == 3 : \n",
    "            raise Exception(\"Comment scraping trials all failed!! :(\")\n",
    "    else:\n",
    "        print(\"Wrong url format given!\")\n",
    "            \n",
    "\n",
    "# Let's test it:      \n",
    "#await _scrape_and_parse_youtube_comments(\"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\", log_ = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypher import Pypher\n",
    "# Scraping transaction : \n",
    "\n",
    "def _scrape_and_store_video_metadata(tx, httpUrl_, metadata_): # \"tx\" is a neo4j transaction...\n",
    "    #q = Pypher()\n",
    "    merges = '\\n'.join([\n",
    "        \"MERGE(t\"+str(i)+':Tag{name:\"'+str(t)+'\"})\\n' + \n",
    "        \"MERGE(t\"+str(i)+\")-[:REFERENCES]->(v)\" \n",
    "        for i, t in enumerate(metadata_['hashtags'])\n",
    "    ]) \n",
    "    #q.CREATE.node('v', labels='Video')\n",
    "    #q.SET.\n",
    "    #for i, t in enumerate(metadata['hashtags']) :\n",
    "    #    q.MERGE.node(\"t\"+str(i))\n",
    "        \n",
    "    result = tx.run(\n",
    "        \"CREATE (v:Video) \"\n",
    "        \"SET v = {title: $title, date: $date, likes: $likes, dislikes:$dislikes, url: $url}\\n\"+merges+\"\\n\"\n",
    "        \"RETURN v.title + ', from node ' + id(v)\", \n",
    "        title=metadata_[\"title\"], \n",
    "        date=metadata_[\"date\"], \n",
    "        likes=metadata_[\"likes\"], \n",
    "        dislikes=metadata_[\"dislikes\"],\n",
    "        url=httpUrl_\n",
    "    )\n",
    "    print('Video Metadata\"', metadata_,'\" sent to database...')\n",
    "    return result\n",
    "\n",
    "def _scrape_and_store_video_comments(tx, httpUrl_, comments_with_authors_):\n",
    "    author_result = []\n",
    "    comment_result = []\n",
    "    for author, comment, likes in comments_with_authors_:\n",
    "        author_result.append(tx.run(\"CREATE (a:Author)\"\n",
    "                  \"SET a = {name: $name}\"\n",
    "                  \"RETURN a.name + ', created as Author with id ' + id(a)\", name=author))\n",
    "        \n",
    "        comment_result.append(tx.run(\n",
    "            \"\"\"\n",
    "                MATCH (v:Video), (a:Author)\n",
    "                WHERE v.url = \"%s\" AND a.name = \"%s\"\n",
    "                CREATE (a) - [r:%s { text: \"%s\", likes: %s }] -> (v)\n",
    "                RETURN v.title, type(r), r.text, a.name\n",
    "            \"\"\" % ( httpUrl_, author, \"COMMENTED\", comment, likes)))\n",
    "    print('Comments send to database...')\n",
    "    return zip(author_result, comment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neo4j for data storage : ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri, user, password = 'bolt://localhost:7687', 'neo4j', 'neo4j_'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================================\n",
      "| SCRAPING VIDEO : https://www.youtube.com/watch?v=lcgqP8g6i84&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\n",
      "======================================================================================\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=lcgqP8g6i84&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\n",
      "Selector loaded: div#info-contents\n",
      "Request finished\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "Video Metadata\" {'title': 'Sentiment Analysis Python - 3 -  Cleaning Text for Natural Language Processing (NLP)', 'date': '04.03.2020', 'hashtags': ['#python', '#nltk', '#nlp'], 'likes': '119', 'dislikes': '2'} \" sent to database...\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2919.83 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=lcgqP8g6i84&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\n",
      "Selector loaded: ytd-comments\n",
      "Request finished\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "['Thank you...', 'Hey! Wonderful explanations, keep up the work man we love it. At the end of the video where you were explaining what it does was really nice because I really needed to understand why there were 2 empty strings and how it managed to remove the punc. Please keep going just a bit in depth because when you understand what you do, you really feel like you learned something and not just copied what a person did.', 'So pumped to code along this tutorial! This is just what I need to fan my python spark after my motivation to code faded a bit.', 'Great video , amazing explanation . Keep this up bro', 'Hey man long time !!!', 'Super bro, you explained it in an easy way...', 'Can u make video on classification of text using python ...ex  ..noun , pronoun etc.. identify from the text file', \"I used regex library. New data = Re.sub('[!@?/:;]', ' ', lower_case)\", \"AttributeError: module 'string' has no attribute 'maketrans'\", 'Bro please put some python project video  please', \"import string  ab='Hello how Are You?' low=ab.lower() #converting text into lowercase punct=low.translate(str.maketrans('','',string.punctuation)) print(low)  it's showing error  'AttributeError: module 'string' has no attribute 'maketrans' '\", 'I just love u..... I learn django from u and now  I am learning nltk ..Nice explanation.... U  know very well what to explain.....N how ......  Thank you soooooooooooooooooooooooooo much...😄😍']\n",
      "Finished parsing\n",
      "Comments send to database...\n"
     ]
    }
   ],
   "source": [
    "httpUrls = [\n",
    "    #\"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\",\n",
    "    #\"https://www.youtube.com/watch?v=Ul0ZgDoamco&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\",\n",
    "    \"https://www.youtube.com/watch?v=lcgqP8g6i84&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\"\n",
    "]\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# resetting database\n",
    "\n",
    "with driver.session() as session:\n",
    "    def _q(query) : return session.run(query)\n",
    "    #---------------------------------------\n",
    "\n",
    "    _q(\"MATCH (n) DETACH DELETE n\") # remove all graphs and nodes! BE CAREFUL!\n",
    "\n",
    "    #---------------------------------------\n",
    "driver.close()\n",
    "\n",
    "with driver.session() as session:\n",
    "    for url in httpUrls :\n",
    "        # run await outside of transaction because asynchronous transactions for Neo4j are not yet available for Python\n",
    "        print(\"\\n======================================================================================\")\n",
    "        print(\"| SCRAPING VIDEO : \"+url)\n",
    "        print(\"======================================================================================\")\n",
    "        metadata = await _scrape_and_parse_video_meta_data(url, log_ = True)\n",
    "        result = session.write_transaction(_scrape_and_store_video_metadata, url, metadata)\n",
    "        #print(result)\n",
    "\n",
    "        comments_with_authors = await _scrape_and_parse_youtube_comments(url, log_ = True)\n",
    "        if len(comments_with_authors) == 0 :\n",
    "            print(\"Video without comments found! This might be wrong!\")\n",
    "            print(\"Let's try again...\")\n",
    "            comments_with_authors = await _scrape_and_parse_youtube_comments(url, log_ = True)\n",
    "            \n",
    "        result = session.write_transaction(_scrape_and_store_video_comments, url, comments_with_authors)\n",
    "        #print(result)\n",
    " \n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
