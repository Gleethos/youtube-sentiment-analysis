{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A Youtube Scaper\n",
    "\n",
    "At the time of writing this, Youtube is the most dominant video streaming platform\n",
    "on the planet, meaning that it enjoys the largest user base producing unimaginable large amounts of content. <br>\n",
    "In this notebook we try to scrape off a tiny bit of Youtube to store it in our database...\n",
    "\n",
    "<img \n",
    "     src=\"images/wall-e.jpg\" \n",
    "     style=\"\n",
    "            float: right;\n",
    "            width: 50%; margin: 2em;\n",
    "            -webkit-box-shadow: 0px 0px 18px -2px rgba(0,0,0,0.75);\n",
    "            -moz-box-shadow: 0px 0px 18px -2px rgba(0,0,0,0.75);\n",
    "            box-shadow: 0px 0px 18px -2px rgba(0,0,0,0.75);\n",
    "     \">\n",
    "\n",
    "\n",
    "We are going to simulate a browser to scrape valuable data such as comments, likes, dislikes and other informations from YouTube videos. <br>\n",
    "\n",
    "Initially, this didn't work at all, because of Google's anti scraping measures. However, after further investigation we managed to bypass their algorithms by using fake browser information and carefully navigating and scrolling on the simulated browser. \n",
    "\n",
    "In essence our scraper tries to \"behaves\" like a real person browsing a Youtube video. <br>\n",
    "The type of user agent and their underlying operating system get selected randomly when starting a video-scrape session. \n",
    "Therefore YouTube can't find repetetive access patterns which are normally displayed by naive web scrapers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyppeteer import launch\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "# for extended documentation visit --> https://miyakogi.github.io/pyppeteer/\n",
    "# !!! function could only be called with await !!!\n",
    "async def scrape(\n",
    "    url_: str, \n",
    "    selector_: str, \n",
    "    page_function_ = \"(element) => element.outerHTML\",\n",
    "    bypass_google_anti_scrape_algorithm_ = False, \n",
    "    log_ = True\n",
    "):\n",
    "    if log_ : print(\"-------------------------Scrape Log Begin--------------------------\", \"\\n\")\n",
    "    #create random user agent so YouTube's algorithm gets pypassed\n",
    "    ua = UserAgent()\n",
    "    agent = ua.random\n",
    "    \n",
    "    # create browser, incognito context and page\n",
    "    browser = await launch(options={\"ping_interval\": None}, ping_interval=None)\n",
    "    context = await browser.createIncognitoBrowserContext()\n",
    "    page = await context.newPage()\n",
    "\n",
    "    await page.bringToFront() # switch to current page (tab switch) --> just for safety\n",
    "    \n",
    "    if log_ : print(\"Browser, Incognito Context and Page created\")\n",
    "    \n",
    "    request_result = \"\"\n",
    "    try:\n",
    "        # set user agent\n",
    "        await page.setUserAgent(agent)\n",
    "        if log_ : print(\"User Agent:\", agent)\n",
    "\n",
    "        # open url\n",
    "        await page.goto(url_)\n",
    "        if log_ : print(\"Url opened:\", url_)\n",
    "        if log_ : print(\"Target:\", selector_)\n",
    "\n",
    "        if bypass_google_anti_scrape_algorithm_:\n",
    "            if log_ : print(\"Info: Start bypassing scrape algorithm\")\n",
    "            await asyncio.gather(\n",
    "                page.waitForSelector(\"h1.title\"),\n",
    "                page.click(\"h1.title\")\n",
    "            )\n",
    "            # multiple scroll to page end to get more comments\n",
    "            for i in range(12):\n",
    "                time.sleep(4)\n",
    "                await asyncio.gather(\n",
    "                    page.keyboard.press(\"End\", delay=20)\n",
    "                )\n",
    "                if log_ : print(\"Scraper Info: scrolled to page end to load more comments. Iteration:\", i + 1)\n",
    "                    \n",
    "            time.sleep(2)\n",
    "\n",
    "        await asyncio.gather(\n",
    "            page.waitForSelector(selector_),\n",
    "            page.click(selector_)\n",
    "        )\n",
    "        if log_ : print(\"Selector loaded:\", selector_)\n",
    "\n",
    "        # get element from query selector and relating function\n",
    "        request_result = await page.querySelectorEval(selector_, page_function_)\n",
    "        if log_ : print(\"Request finished\")\n",
    "    except Exception as e:\n",
    "        print('WARNING! : Error in \"scrape\" method! Exception:',e)\n",
    "        raise Exception(str(e)) \n",
    "    finally:\n",
    "        await page.close()\n",
    "        if log_ : print(\"Page closed\")\n",
    "        await context.close()\n",
    "        if log_ : print(\"Incognito context closed\")\n",
    "        # close browser\n",
    "        await browser.disconnect()\n",
    "        await browser.close()\n",
    "        if log_ : print(\"Browser closed\", \"\\n\")\n",
    "        if log_ : print(\"-------------------------Scrape Log End----------------------------\", \"\\n\")\n",
    "    \n",
    "    return request_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test the \"scrape\" method: ##\n",
    "\n",
    "As you can see our scrape method can be used to download any type of content of any webpage. \n",
    "\n",
    "It has the beneave listed parameters:\n",
    "- (necessary) - `url` the url where the YouTube video is registered.\n",
    "- (optional) - `selector_`: In essence, this is just a CSS selector like \"h1.title\". It would match the following: `<h1 class=\"title\"></h1>`.\n",
    "- (optional) - `page_function_`: A JavaScript lambda expression applied to each matched element. Example given: `(element) => element.firstChild.innerHTML`.\n",
    "- (optional) - `bypass_google_anti_scrape_algorithm_` - This is just a boolean flag whether to bypass the anti scraping algorithm or not.\n",
    "- (optional) - `log_`: This flag just enables logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get YouTube Video Title\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\"\n",
    "query_selector = \"h1.title\"\n",
    "function = \"(element) => element.firstChild.innerHTML\"\n",
    "\n",
    "#title = await scrape(url, query_selector, function)                    \n",
    "#print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides scraping video metadata like the title, we can also use out `scrape` method for scraping comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get comments and their authors as html\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\"\n",
    "query_selector = \"ytd-comments\"\n",
    "function = \"(element) => element.outerHTML\"\n",
    "\n",
    "#html = await scrape(url, query_selector, function, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "<img \n",
    "     src=\"images/web-scraper-5000.webp\" \n",
    "     style=\"\n",
    "            float: right;\n",
    "            width: 40%; margin: 2em;\n",
    "            -webkit-box-shadow: 0px 0px 18px -2px rgba(0,0,0,0.75);\n",
    "            -moz-box-shadow: 0px 0px 18px -2px rgba(0,0,0,0.75);\n",
    "            box-shadow: 0px 0px 18px -2px rgba(0,0,0,0.75);\n",
    "     \">\n",
    "     \n",
    "The `scrape` method works! <br>\n",
    "We can pass it a great number of parameters to fully customize whatever scraping process we require.\n",
    "<br>\n",
    "We can now scrape off any html data we want from any youtube url! :)\n",
    "\n",
    "This new method will be a central component of the following methods\n",
    "which will extract relevant information from the html we reveive by the scraper method..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play around with BeautifulSoup for html parsing: ##\n",
    "\n",
    "Due to the fact, that we get HTML code as a response from our scraper, we need to extract the relevant data out of it.<br>\n",
    "The following method extracts comments and their corresponding authors out of the raw html from the comment section of a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parse html and assign them\n",
    "\n",
    "def _parse_comments_with_corresponding_authors(html_, log_ = True):\n",
    "    soup = BeautifulSoup(html_, features=\"html.parser\")\n",
    "\n",
    "    # get authors of comments and clear html data\n",
    "    authors = [item.text.strip() for item in soup.select(\"a[id=author-text] > span\")]\n",
    "\n",
    "    # get comments and clear html data\n",
    "    comments = [\n",
    "        item.text.strip().replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").replace(\"\\\"\", \"'\") \n",
    "        for item in soup.select(\"yt-formatted-string[id=content-text]\")\n",
    "    ]\n",
    "    #print(comments)\n",
    "\n",
    "    likes = [\n",
    "        item.text.strip().replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").replace(\".\",\"\").replace(\",\",\"\")\n",
    "        for item in soup.select(\"span[id=vote-count-middle]\")\n",
    "    ]\n",
    "    \n",
    "    #<span id=\"vote-count-middle\" class=\"style-scope ytd-comment-action-buttons-renderer\" aria-label=\"2&nbsp;&quot;Mag ich&quot;-Bewertungen\">\n",
    "    #print(likes)\n",
    "    comments_with_authors_and_likes = list(zip(authors, comments, likes))\n",
    "\n",
    "    \n",
    "    if log_:\n",
    "        print(\"Finished parsing\")\n",
    "        #for author, comment, likes in comments_with_authors_and_likes:\n",
    "        #    print(author, \"wrote:\\n -\" + comment + \" with \"+likes+\" likes\")\n",
    "    \n",
    "    return comments_with_authors_and_likes\n",
    "\n",
    "# Let's try it:\n",
    "#_parse_comments_with_corresponding_authors(html, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build parser methods: ##\n",
    "\n",
    "The following method defines an algorithm which scrapes video metadata like \n",
    "the name, likes, dislikes, date and so on... <br>\n",
    "\n",
    "Internally, it calls the `scrape()` method to get the raw HTML code from the given url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape and parse video metadata\n",
    "\n",
    "# returns metadata as dict\n",
    "# function is asynchronous and therefore it has to be awaited\n",
    "async def _scrape_and_parse_video_meta_data(url: str, log_ = True):\n",
    "    if \"youtube.com\" in url:\n",
    "        \n",
    "        trials = 1\n",
    "        max_trials = 5\n",
    "        \n",
    "        while trials < max_trials:\n",
    "            try:\n",
    "                html = await scrape(\n",
    "                    url, \n",
    "                    \"div#info-contents\",\n",
    "                    \"(element) => element.outerHTML\", \n",
    "                    bypass_google_anti_scrape_algorithm_ = (trials > 2),\n",
    "                    log_ = log_\n",
    "                )\n",
    "                \n",
    "                trials = 100000\n",
    "            except Exception as e:\n",
    "                print('WARNING! : Metadata Scraping trial',trials,'failed for url=\"',url,'\"!')\n",
    "                print('ERROR:\\n',e)\n",
    "                print('Trying again...')\n",
    "                trials = trials + 1\n",
    "                \n",
    "        if trials == max_trials : \n",
    "            raise Exception(\"Meta-Data scraping trials all failed!! :(\")\n",
    "       \n",
    "        soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "        title = soup.find(\"h1\", {\"class\": \"title\"}).find(\"yt-formatted-string\").text\n",
    "        primary_info = soup.find_all(\"yt-formatted-string\", {\"class\": \"ytd-video-primary-info-renderer\"})\n",
    "        \n",
    "        date = (primary_info[len(primary_info) - 1].text)\n",
    "    \n",
    "        hashtags = [ tag.text.strip() for tag in primary_info[0].find_all(\"a\") if tag != None]\n",
    "        \n",
    "        likes = soup.select(\"yt-formatted-string[id=text]\")[0].text.replace(\".\", \"\").replace(\",\",\"\").replace(\"\\xa0Mio\",\"0\"*6)\n",
    "        dislikes = soup.select(\"yt-formatted-string[id=text]\")[1].text.replace(\".\", \"\").replace(\",\",\"\").replace(\"\\xa0Mio\",\"0\"*6)\n",
    "        \n",
    "        return {\"title\": title, \"date\": date, \"hashtags\": hashtags, \"likes\": likes, \"dislikes\": dislikes}\n",
    "    else:\n",
    "        print(\"Wrong url format given!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to gather the comments for a specific video! <br>\n",
    "The following method does exactly that.\n",
    "It takes a video url and runs the `scrape()` method internally and afterwards parses it just like the previous method... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape and parse comments with authors\n",
    "\n",
    "# returns list of tuples [(Author, Comment), (...), ...]\n",
    "# function is asynchronous and therefore it has to be awaited\n",
    "async def _scrape_and_parse_youtube_comments(url: str, log_ = True):\n",
    "    \n",
    "    if \"youtube.com\" in url:\n",
    "        trials = 1\n",
    "        max_trials = 5\n",
    "        \n",
    "        while trials < max_trials:\n",
    "            try:\n",
    "                html = await scrape(url, \"ytd-comments\", \"(element) => element.outerHTML\", bypass_google_anti_scrape_algorithm_ = True, log_ = log_)\n",
    "        \n",
    "                return _parse_comments_with_corresponding_authors(html, log_ = log_)\n",
    "        \n",
    "            except Exception as e:\n",
    "                print('WARNING! : Comment scraping trial',trials,'failed for url=\"',url,'\"!')\n",
    "                print('ERROR:\\n',e)\n",
    "                print('Trying again...')\n",
    "                trials = trials + 1\n",
    "\n",
    "        if trials == max_trials : \n",
    "            raise Exception(\"Comment scraping trials all failed!! :(\")\n",
    "    else:\n",
    "        print(\"Wrong url format given!\")\n",
    "            \n",
    "\n",
    "# Let's test it:      \n",
    "#await _scrape_and_parse_youtube_comments(\"https://www.youtube.com/watch?v=dyN_WtjdfpA&list=PLhTjy8cBISEoOtB5_nwykvB9wfEDscuEo\", log_ = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing scraped Data as a Graph: ###\n",
    "\n",
    "Now we need to store the data into the database! <br>\n",
    "This is done via the following two methods.\n",
    "The first one handles storing video metadata and the second one\n",
    "stores the video comments. <br>\n",
    "As you can see we user cypher queries which will be send to the Neo4j database. More information on cypher: https://neo4j.com/docs/cypher-manual/current/\n",
    "\n",
    "#### Data Structure: ####\n",
    "\n",
    "We want to save to save the data in a graph structure, which contains the following nodes and their respective attributes:\n",
    "- `Video`: id, url, title, likes and dislikes (general video metadata)\n",
    "- `Author`: id, name (a YouTube user who commented on a video)\n",
    "- `Tag`: id, name (video hashtag)\n",
    "\n",
    "To get more valuable data, we connected these nodes through the following relations:\n",
    "- `COMMENTED`: id, text, likes, score (connects authors to videos on which they commented)\n",
    "- `REFERENCES`: id (connects tags to videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping transaction : \n",
    "\n",
    "def _store_video_metadata(tx, httpUrl_, metadata_): # \"tx\" is a neo4j transaction...\n",
    "     \n",
    "    merges = '\\n'.join([\n",
    "        \"MERGE(t\"+str(i)+':Tag{name:\"'+str(t).replace('\"','')+'\"})\\n' + \n",
    "        \"MERGE(t\"+str(i)+\")-[:REFERENCES]->(v)\" \n",
    "        for i, t in enumerate(metadata_['hashtags'])\n",
    "    ])\n",
    "    \n",
    "    # extract date from string, Example: 'Premiere am 06.12.2018'\n",
    "    metadata_[\"date\"] = metadata_[\"date\"].split()\n",
    "    metadata_[\"date\"] = metadata_[\"date\"][len(metadata_[\"date\"]) - 1]\n",
    "    \n",
    "    result = tx.run(\n",
    "        \"MERGE (v:Video {url: $url}) \"\n",
    "        \"SET v = {title: $title, date: $date, likes: $likes, dislikes: $dislikes, url: $url}\"\n",
    "        \"\\n\"+merges+\"\\n\"\n",
    "        \"RETURN v.title + ', from node ' + id(v)\", \n",
    "        title=metadata_[\"title\"], \n",
    "        date=metadata_[\"date\"],\n",
    "        likes=metadata_[\"likes\"], \n",
    "        dislikes=metadata_[\"dislikes\"],\n",
    "        url=httpUrl_\n",
    "    )\n",
    "    print('Video Metadata', metadata_,' sent to database...')\n",
    "    return result\n",
    "\n",
    "def _store_video_comments(tx, httpUrl_, data):\n",
    "    comments_with_authors_, ratio = data\n",
    "    author_result = []\n",
    "    comment_result = []\n",
    "    for author, comment, likes in comments_with_authors_:\n",
    "        author_result.append(tx.run(\n",
    "            \"MERGE (a:Author{name: $name})\"\n",
    "            \"RETURN a.name + ', created as Author with id ' + id(a)\", \n",
    "            name=author\n",
    "        )) \n",
    "        comment_result.append(tx.run(\n",
    "            \"MATCH (v:Video), (a:Author) \"\n",
    "            \"WHERE v.url = $url AND a.name = $name \"\n",
    "            \"MERGE (a) - [r: COMMENTED {text: $comment_text}] -> (v) \"\n",
    "            \"SET r = {text: $comment_text, likes: $likes, score: $score } \"\n",
    "            \"RETURN v.title, type(r), r.text, a.name ;\",\n",
    "            url=httpUrl_,\n",
    "            name=author, \n",
    "            comment_text=comment,\n",
    "            likes=likes, \n",
    "            score=ratio*int(likes)\n",
    "        ))\n",
    "        \n",
    "    print('Comments sent to database...')\n",
    "    return zip(author_result, comment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neo4j for data storage : ##\n",
    "\n",
    "Before starting to scrape some urls we need to think about how to store our data. <br>\n",
    "Youtube is a social media platform hosting data which is very associative and sparse in nature.\n",
    "Therefore it is best stored via a **graph database**! <br>\n",
    "We are going to use Neo4j to store the scraped data as it can handle such data best! <br>\n",
    "Let's import it and set up the credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri, user, password = 'bolt://localhost:7687', 'neo4j', 'neo4j_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything set up we want to \n",
    "go through a list of video urls and scrape them for their data! :)<br>\n",
    "All we need to do is call the previously defined methods for scraping and storing urls...\n",
    "\n",
    "However, after some time working on this notebook we gathered a lot of youtube video urls... <br>\n",
    "Listing all of them in this notebook would be ridiculously ugly and distracting! <br>\n",
    "Therefore, we stored them in a simple text file located in the `data_sources` folder.\n",
    "We load this file via the following code and generate a list of all url strings.\n",
    "Then we remove duplicate entries by converting it temporarily into a set.\n",
    "In order to get rid of the indeterministic randomenss produced by the set\n",
    "we then sort the list of strings and then we shuffle the list based on a seed for predictive pseudo randomness..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three urls: ['https://www.youtube.com/watch?v=nIR2GR254S8', 'https://www.youtube.com/watch?v=EzMaS7TodE4', 'https://www.youtube.com/watch?v=IUDTlvagjJA'] ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# start get urls from files\n",
    "def all_files_at(path):\n",
    "    files = []\n",
    "    for file in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, file)): \n",
    "            files.extend(all_files_at(os.path.join(path, file)))\n",
    "        else:\n",
    "            files.append(os.path.join(path, file))\n",
    "    return files\n",
    "            \n",
    "httpUrls = []\n",
    "\n",
    "# read get urls from files and store them in a list\n",
    "for file in all_files_at('data_sources'):\n",
    "    with open(file) as openfileobject:\n",
    "        for line in openfileobject:\n",
    "            httpUrls.append(line.strip())\n",
    "\n",
    "# removing duplicate videos if some exist ... and sort them\n",
    "httpUrls = sorted(list(set(httpUrls))) \n",
    "\n",
    "import random\n",
    "random.seed(66642999)\n",
    "random.shuffle(httpUrls) # Now we shuffle the urls so that the data is (pseudo) randomly stored!\n",
    "\n",
    "# let's take a look\n",
    "print(\"First three urls:\", httpUrls[:3], \"...\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The list of youtube urls is now ready for scraping. <br>\n",
    "Before iterating over the `httpUrls` list we first clear the database.<br>\n",
    "The query below is commented out by default because we want to keep data that\n",
    "has already been scraped in previous sessions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# optionally resetting database by uncommenting the following query...\n",
    "with driver.session() as session:\n",
    "    def _q(query) : return session.run(query) \n",
    "    #_q(\"MATCH (n) DETACH DELETE n\") # remove all graphs and nodes! BE CAREFUL!\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block is the actual scraping loop which iterates over\n",
    "all the url links we loaded into the `httpUrls` list... <br> \n",
    "\n",
    "At the beginning of an iteration we first and foremost check if the current url\n",
    "is already stored in the database! <br>\n",
    "This would mean that the video has already been scraped and therefore the exhaustive\n",
    "scraping procedure does not need to be repeated again for that video...<br>\n",
    "\n",
    "Them we proceed with calling the scrape methods surounded by `try` & `except` blocks so that \n",
    "possible failed scraping attempts do not shut down the entire process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video with url https://www.youtube.com/watch?v=nIR2GR254S8 already exists in database. Skipping ... ( 0 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=EzMaS7TodE4 already exists in database. Skipping ... ( 1 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=IUDTlvagjJA already exists in database. Skipping ... ( 2 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=sVJO1wB0ZHk already exists in database. Skipping ... ( 3 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=Iot0eF6EoNA already exists in database. Skipping ... ( 4 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=q8dXBLQLzeQ already exists in database. Skipping ... ( 5 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=qjJcomKo_O8 already exists in database. Skipping ... ( 6 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=N_7eYLpynOs already exists in database. Skipping ... ( 7 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=ulsQpeEUxx0 already exists in database. Skipping ... ( 8 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=vfc42Pb5RA8 already exists in database. Skipping ... ( 9 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=hp_6u_MjjJ0 already exists in database. Skipping ... ( 10 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=3x1b_S6Qp2Q already exists in database. Skipping ... ( 11 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=hSlb1ezRqfA already exists in database. Skipping ... ( 12 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=kffacxfA7G4 already exists in database. Skipping ... ( 13 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=ciNHn38EyRc already exists in database. Skipping ... ( 14 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=u1V-7UiXNZg already exists in database. Skipping ... ( 15 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=FJ3oHpup-pk already exists in database. Skipping ... ( 16 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=gFEjCuae8RA already exists in database. Skipping ... ( 17 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=wJuI5zKfKrg already exists in database. Skipping ... ( 18 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=tpiyEe_CqB4 already exists in database. Skipping ... ( 19 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=ec65fZXcWv4 already exists in database. Skipping ... ( 20 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=FlsCjmMhFmw already exists in database. Skipping ... ( 21 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=CZIt20emgLY already exists in database. Skipping ... ( 22 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=5lXNotvNuiU already exists in database. Skipping ... ( 23 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=Ul0ZgDoamco already exists in database. Skipping ... ( 24 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=DxZVfwudvC8 already exists in database. Skipping ... ( 25 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=i9pNYW1Pg9A already exists in database. Skipping ... ( 26 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=rRAc29qoywA already exists in database. Skipping ... ( 27 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=AFCGjW6Bwjs already exists in database. Skipping ... ( 28 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=L0H6xYwMQnk already exists in database. Skipping ... ( 29 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=1RwQlgS4zQI already exists in database. Skipping ... ( 30 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=Gc3ZfqEBW7A already exists in database. Skipping ... ( 31 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=7plsgTxJn88 already exists in database. Skipping ... ( 32 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=4gSOMba1UdM already exists in database. Skipping ... ( 33 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=HLF29w6YqXs already exists in database. Skipping ... ( 34 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=MXJ-zpJeY3E already exists in database. Skipping ... ( 35 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=XmIIgE7eSak already exists in database. Skipping ... ( 36 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=7rNY7xKyGCQ already exists in database. Skipping ... ( 37 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=ffKL4LmA-_4 already exists in database. Skipping ... ( 38 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=ASFSXNQKPDI already exists in database. Skipping ... ( 39 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=w3ugHP-yZXw already exists in database. Skipping ... ( 40 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=cbqft9HbmDE already exists in database. Skipping ... ( 41 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=_pLTNTyD6nw already exists in database. Skipping ... ( 42 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=1oN5v0mcTMs already exists in database. Skipping ... ( 43 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=BiqDZlAZygU already exists in database. Skipping ... ( 44 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=PmLJSnHxQfg already exists in database. Skipping ... ( 45 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=0jspaMLxBig already exists in database. Skipping ... ( 46 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=C1vW9iSpLLk already exists in database. Skipping ... ( 47 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=VoUD5MDfCEM already exists in database. Skipping ... ( 48 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=o5N-qPEnThs already exists in database. Skipping ... ( 49 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=i4XvlM_j3A0 already exists in database. Skipping ... ( 50 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=LCCL9W9gwu0 already exists in database. Skipping ... ( 51 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=N274EurzpAA already exists in database. Skipping ... ( 52 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=1zZZjaYl4AA already exists in database. Skipping ... ( 53 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=tQ_nVSxjn_s already exists in database. Skipping ... ( 54 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=eIrMbAQSU34 already exists in database. Skipping ... ( 55 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=NoQil9EWjxo already exists in database. Skipping ... ( 56 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=4QEKV6cQxCI already exists in database. Skipping ... ( 57 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=Bd0cMmBvqWc already exists in database. Skipping ... ( 58 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=2lAe1cqCOXo already exists in database. Skipping ... ( 59 of 131 Videos scraped.)\n",
      "\n",
      "|===========================================================================================================|\n",
      "| STARTING --> SCRAPING, PARSING AND STORING VIDEO : https://www.youtube.com/watch?v=ufnTXNoXAqw\n",
      "|===========================================================================================================|\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Url opened: https://www.youtube.com/watch?v=ufnTXNoXAqw\n",
      "Target: div#info-contents\n",
      "WARNING! : Error in \"scrape\" method! Exception: No node found for selector: div#info-contents\n",
      "Page closed\n",
      "Incognito context closed\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "WARNING! : Metadata Scraping trial 1 failed for url=\" https://www.youtube.com/watch?v=ufnTXNoXAqw \"!\n",
      "ERROR:\n",
      " No node found for selector: div#info-contents\n",
      "Trying again...\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.2 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=ufnTXNoXAqw\n",
      "Target: div#info-contents\n",
      "Selector loaded: div#info-contents\n",
      "Request finished\n",
      "Page closed\n",
      "Incognito context closed\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "Video Metadata {'title': 'What Will Happen to Trump After The Storming of the Capitol?', 'date': '12.01.2021', 'hashtags': ['#1593'], 'likes': '18006', 'dislikes': '13371'}  sent to database...\n",
      "Starting scraping and storing comments...\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2919.83 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=ufnTXNoXAqw\n",
      "Target: ytd-comments\n",
      "Info: Start bypassing scrape algorithm\n",
      "WARNING! : Error in \"scrape\" method! Exception: No node found for selector: h1.title\n",
      "Page closed\n",
      "Incognito context closed\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "WARNING! : Comment scraping trial 1 failed for url=\" https://www.youtube.com/watch?v=ufnTXNoXAqw \"!\n",
      "ERROR:\n",
      " No node found for selector: h1.title\n",
      "Trying again...\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.2 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=ufnTXNoXAqw\n",
      "Target: ytd-comments\n",
      "Info: Start bypassing scrape algorithm\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 1\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 2\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 3\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 4\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 5\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 6\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 7\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 8\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 9\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 10\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 11\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 12\n",
      "Selector loaded: ytd-comments\n",
      "Request finished\n",
      "Page closed\n",
      "Incognito context closed\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "Finished parsing\n",
      "Comments sent to database...\n",
      "Info: 61 of 131 Videos scraped.\n",
      "|==================================================|\n",
      "| FINISHED --> SCRAPING, PARSING AND STORING VIDEO |\n",
      "|==================================================|\n",
      "\n",
      "Video with url https://www.youtube.com/watch?v=qJW0LUE1RUA already exists in database. Skipping ... ( 61 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=dyN_WtjdfpA already exists in database. Skipping ... ( 62 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=YbJOTdZBX1g already exists in database. Skipping ... ( 63 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=Q2VSOQvz6o8 already exists in database. Skipping ... ( 64 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=XUyoJ79P2SQ already exists in database. Skipping ... ( 65 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=SzJ46YA_RaA already exists in database. Skipping ... ( 66 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=QwZT7T-TXT0 already exists in database. Skipping ... ( 67 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=cfDAdI2r-fw already exists in database. Skipping ... ( 68 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=c13vkeVS4MM already exists in database. Skipping ... ( 69 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=QQ9gs-5lRKc already exists in database. Skipping ... ( 70 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=IfHG7bj-CEI already exists in database. Skipping ... ( 71 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=lcgqP8g6i84 already exists in database. Skipping ... ( 72 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=3o3imqMkOYs already exists in database. Skipping ... ( 73 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=OSjBTegtHqE already exists in database. Skipping ... ( 74 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=MO7erSNZszI already exists in database. Skipping ... ( 75 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=pKO9UjSeLew already exists in database. Skipping ... ( 76 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=KrEhmADXTr8 already exists in database. Skipping ... ( 77 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=OmJ-4B-mS-Y already exists in database. Skipping ... ( 78 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=BgJAA4xx2D4 already exists in database. Skipping ... ( 79 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=Sw9r8CL98N0 already exists in database. Skipping ... ( 80 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=IT__Nrr3PNI already exists in database. Skipping ... ( 81 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=rB83DpBJQsE already exists in database. Skipping ... ( 82 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=N39o_DI5laI already exists in database. Skipping ... ( 83 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=75U0YdxIi8M already exists in database. Skipping ... ( 84 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=EeF3UTkCoxY already exists in database. Skipping ... ( 85 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=L2c9q6zVp-I already exists in database. Skipping ... ( 86 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=_DPRt3AcUEY already exists in database. Skipping ... ( 87 of 131 Videos scraped.)\n",
      "\n",
      "|===========================================================================================================|\n",
      "| STARTING --> SCRAPING, PARSING AND STORING VIDEO : https://www.youtube.com/watch?v=4b33NTAuF5E\n",
      "|===========================================================================================================|\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.124 Safari/537.36\n",
      "WARNING! : Error in \"scrape\" method! Exception: Navigation Timeout Exceeded: 30000 ms exceeded.\n",
      "Page closed\n",
      "Incognito context closed\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "WARNING! : Metadata Scraping trial 1 failed for url=\" https://www.youtube.com/watch?v=4b33NTAuF5E \"!\n",
      "ERROR:\n",
      " Navigation Timeout Exceeded: 30000 ms exceeded.\n",
      "Trying again...\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2866.71 Safari/537.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Url opened: https://www.youtube.com/watch?v=4b33NTAuF5E\n",
      "Target: div#info-contents\n",
      "WARNING! : Error in \"scrape\" method! Exception: No node found for selector: div#info-contents\n",
      "Page closed\n",
      "Incognito context closed\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "WARNING! : Metadata Scraping trial 2 failed for url=\" https://www.youtube.com/watch?v=4b33NTAuF5E \"!\n",
      "ERROR:\n",
      " No node found for selector: div#info-contents\n",
      "Trying again...\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=4b33NTAuF5E\n",
      "Target: div#info-contents\n",
      "Info: Start bypassing scrape algorithm\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 1\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 2\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 3\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 4\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 5\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 6\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 7\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 8\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 9\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 10\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 11\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 12\n",
      "Selector loaded: div#info-contents\n",
      "Request finished\n",
      "Page closed\n",
      "Incognito context closed\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "Video Metadata {'title': 'Can You Upload Your Mind & Live Forever? feat. Cyberpunk 2077', 'date': '10.12.2020', 'hashtags': [], 'likes': '344949', 'dislikes': '5290'}  sent to database...\n",
      "Starting scraping and storing comments...\n",
      "-------------------------Scrape Log Begin-------------------------- \n",
      "\n",
      "Browser, Incognito Context and Page created\n",
      "User Agent: Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.17 Safari/537.36\n",
      "Url opened: https://www.youtube.com/watch?v=4b33NTAuF5E\n",
      "Target: ytd-comments\n",
      "Info: Start bypassing scrape algorithm\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 1\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 2\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 3\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 4\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 5\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 6\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 7\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 8\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 9\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 10\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 11\n",
      "Scraper Info: scrolled to page end to load more comments. Iteration: 12\n",
      "Selector loaded: ytd-comments\n",
      "Request finished\n",
      "Page closed\n",
      "Incognito context closed\n",
      "Browser closed \n",
      "\n",
      "-------------------------Scrape Log End---------------------------- \n",
      "\n",
      "Finished parsing\n",
      "Comments sent to database...\n",
      "Info: 89 of 131 Videos scraped.\n",
      "|==================================================|\n",
      "| FINISHED --> SCRAPING, PARSING AND STORING VIDEO |\n",
      "|==================================================|\n",
      "\n",
      "Video with url https://www.youtube.com/watch?v=IvxRsDpXPGo already exists in database. Skipping ... ( 89 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=LKOl8LoXPSY already exists in database. Skipping ... ( 90 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=yufg9PdxrWs already exists in database. Skipping ... ( 91 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=X6p5AZp7r_Q already exists in database. Skipping ... ( 92 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=kfVsfOSbJY0 already exists in database. Skipping ... ( 93 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=o0vD7vqP1M8 already exists in database. Skipping ... ( 94 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=M0O7lLe4SmA already exists in database. Skipping ... ( 95 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=ueNT-w7Oluw already exists in database. Skipping ... ( 96 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=WFcGOHEAypM already exists in database. Skipping ... ( 97 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=uaPrrg4tytE already exists in database. Skipping ... ( 98 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=OTfp2_SwxHk already exists in database. Skipping ... ( 99 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=Qd-CwJa1SHE already exists in database. Skipping ... ( 100 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=RZEb_utxH4s already exists in database. Skipping ... ( 101 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=MAlSjtxy5ak already exists in database. Skipping ... ( 102 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=Iyt8rYpUQsg already exists in database. Skipping ... ( 103 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=CAyWN9ba9J8 already exists in database. Skipping ... ( 104 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=jey_CzIOfYE already exists in database. Skipping ... ( 105 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=IlU-zDU6aQ0 already exists in database. Skipping ... ( 106 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=kTXTPe3wahc already exists in database. Skipping ... ( 107 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=HbaIfdEAkII already exists in database. Skipping ... ( 108 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=067wXeqPCQU already exists in database. Skipping ... ( 109 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=t-h1BhO4V_U already exists in database. Skipping ... ( 110 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=HtYweBOCp7A already exists in database. Skipping ... ( 111 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=N5wW-FueKBI already exists in database. Skipping ... ( 112 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=pbUXfdUUEqM already exists in database. Skipping ... ( 113 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=pOdrPruSnrw already exists in database. Skipping ... ( 114 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=wx9Jv5uxfac already exists in database. Skipping ... ( 115 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=DqgDqFrBnlQ already exists in database. Skipping ... ( 116 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=vgxCnXdHi68 already exists in database. Skipping ... ( 117 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=8QO0OxWUxpo already exists in database. Skipping ... ( 118 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=1vWTJzJx0i4 already exists in database. Skipping ... ( 119 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=lBloZhNyCcE already exists in database. Skipping ... ( 120 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=lhjRXO72v1s already exists in database. Skipping ... ( 121 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=b6Yki9Pr-7k already exists in database. Skipping ... ( 122 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=ggyVXHU0_ms already exists in database. Skipping ... ( 123 of 131 Videos scraped.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video with url https://www.youtube.com/watch?v=cV_uKzKZsHs already exists in database. Skipping ... ( 124 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=3zhCbaVBEjg already exists in database. Skipping ... ( 125 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=2a-CVGbUUjQ already exists in database. Skipping ... ( 126 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=OiPVgAhRE6E already exists in database. Skipping ... ( 127 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=uTxRF5ag27A already exists in database. Skipping ... ( 128 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=eyfcaso25N8 already exists in database. Skipping ... ( 129 of 131 Videos scraped.)\n",
      "Video with url https://www.youtube.com/watch?v=3X1tzvhagH8 already exists in database. Skipping ... ( 130 of 131 Videos scraped.)\n",
      "Whole scraping finished. Results:\n",
      "Scraped videos: 131\n",
      "Unscraped videos: 0\n",
      "The Following videos were not scraped: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# activate logging for more details\n",
    "log_process = True\n",
    "\n",
    "counter = 0\n",
    "error_urls = []\n",
    "\n",
    "with driver.session() as session:\n",
    "    for url in httpUrls :\n",
    "        \n",
    "        # |--------------------------------------------------------------------------------\n",
    "        # | start\n",
    "        # | Videos which are already in the database do not need to be scraped multiple times\n",
    "        # | Therefore, we check if a video with that url already exists\n",
    "        # |--------------------------------------------------------------------------------\n",
    "        \n",
    "        video_exists = False\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            def _q(query) : return session.run(query)\n",
    "            #---------------------------------------            \n",
    "            result = _q(\"MATCH (v:Video {url: '\" + url + \"'}) RETURN count(v)\")\n",
    "            for record in result:\n",
    "                if record[\"count(v)\"] >= 1:\n",
    "                    video_exists = True\n",
    "            #---------------------------------------\n",
    "        driver.close()\n",
    "        \n",
    "        if video_exists :\n",
    "            print(\"Video with url\", url, \"already exists in database. Skipping ...\", \"(\",counter, \"of\", len(httpUrls), \"Videos scraped.)\")\n",
    "            counter = counter + 1\n",
    "            continue\n",
    "            \n",
    "        # |--------------------------------------------------------------------------------\n",
    "        # | end \n",
    "        # |--------------------------------------------------------------------------------\n",
    "        \n",
    "        # run await outside of transaction because asynchronous transactions for Neo4j are not yet available for Python\n",
    "        print()\n",
    "        print(\"|===========================================================================================================|\")\n",
    "        print(\"| STARTING --> SCRAPING, PARSING AND STORING VIDEO : \" + url)\n",
    "        print(\"|===========================================================================================================|\")\n",
    "        try:\n",
    "            metadata = await _scrape_and_parse_video_meta_data(url, log_ = log_process)\n",
    "            result = session.write_transaction(_store_video_metadata, url, metadata)\n",
    "            print(\"Starting scraping and storing comments...\")\n",
    "\n",
    "            comments_with_authors = await _scrape_and_parse_youtube_comments(url, log_ = log_process)\n",
    "            if len(comments_with_authors) == 0 :\n",
    "                print(\"Video without comments found! This might be wrong!\")\n",
    "                print(\"Let's try again...\")\n",
    "                comments_with_authors = await _scrape_and_parse_youtube_comments(url, log_ = log_process)\n",
    "\n",
    "            ratio = (int(metadata['likes']))/(int(metadata['likes'])+int(metadata['dislikes']))*2 - 1 \n",
    "\n",
    "            data = (comments_with_authors, ratio)\n",
    "            result = session.write_transaction(_store_video_comments, url, data)\n",
    "            #print(result)\n",
    "            \n",
    "            counter = counter + 1\n",
    "            print(\"Info:\", counter, \"of\", len(httpUrls), \"Videos scraped.\")            \n",
    "        except Exception as e:\n",
    "            print('Failed loading url:', url)\n",
    "            print(\"Error:\", \"\\n\", e)\n",
    "            error_urls.append(url)\n",
    "            print(\"Videos which failed to be scraped:\", error_urls, \"\\n\")\n",
    "    \n",
    "        print(\"|==================================================|\")\n",
    "        print(\"| FINISHED --> SCRAPING, PARSING AND STORING VIDEO |\")  \n",
    "        print(\"|==================================================|\")\n",
    "        print()\n",
    "            \n",
    "    print(\"Whole scraping finished. Results:\")\n",
    "    print(\"Scraped videos:\", counter)\n",
    "    print(\"Unscraped videos:\", len(error_urls))\n",
    "    print(\"The Following videos were not scraped:\", error_urls)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
